# Faster-whisper-Kaggle-Version-with-2-T4-GPU
åª’é«”ã€æ”¿æ²»å·¥ä½œè€…çš„é•·ç¯‡é€å­—ç¨¿æ•‘æ˜Ÿ

<div align="center">
  <a href="#ç¹é«”ä¸­æ–‡" style="margin-right: 20px;">
    <img src="https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-000?style=for-the-badge&logo=translate&logoColor=white" alt="ç¹é«”ä¸­æ–‡">
  </a>
  <a href="#english">
    <img src="https://img.shields.io/badge/English-000?style=for-the-badge&logo=translate&logoColor=white" alt="English">
  </a>
</div>

---
## ç¹é«”ä¸­æ–‡

### æœ¬å°ˆæ¡ˆæä¾›åª’é«”å·¥ä½œè€…ã€æ”¿æ²»å·¥ä½œè€…åŠè‹¦é€¼å·¥è®€ç”Ÿæ“ºè„«ä¸æ•ˆç‡çš„**è½æ‰“å›°å¢ƒ**ï¼Œç‰¹åˆ¥è¨­è¨ˆä¸€å€‹é«˜æ•ˆçš„éŸ³é »è½‰éŒ„çš„å‚»ç“œæ“ä½œæ–¹æ¡ˆã€‚
### ç‚ºä½•ä¸ä½¿ç”¨colabï¼Ÿ
1. å› ç‚ºç²å–GPUçš„é™åˆ¶èˆ‡ä¸ç©©å®šæ€§ï¼Œå³ä¾¿æ›è¼‰driveå¯ä»¥è®“æ•´å€‹ç¨‹åºæ›´å®¹æ˜“ä½¿ç”¨ï¼Œä½†è½‰éŒ„ä»»å‹™çš„é‡é»ä»åœ¨GPUã€‚
2. å› æ­¤ï¼Œç‚ºäº†å……åˆ†åˆ©ç”¨æ‰€æœ‰å…è²»è³‡æºï¼Œä½¿ç”¨ Kaggle å¹³å°å…è²»æä¾›çš„å…©å¼µ T4 GPU è³‡æºï¼Œå¯ä»¥å¤§å¹…æå‡è½‰éŒ„æ•ˆç‡ï¼ŒåŠ©æ‚¨è¼•é¬†å®Œæˆå„é¡è½‰éŒ„ä»»å‹™ã€‚**[æ›´æ–°]** **æ–°ä»£ç¢¼æœƒè‡ªå‹•åµæ¸¬å¯ç”¨çš„ GPU æ•¸é‡ï¼Œä¸¦åˆç†åˆ†é…ä»»å‹™ã€‚**
3. kaggleæ˜¯ä¸€å€‹å…è²»çš„æ©Ÿå™¨å­¸ç¿’å¹³å°ï¼Œç‰¹é»æ˜¯æ¯å€‹æ˜ŸæœŸæä¾›30å€‹å°æ™‚çš„å…è²»GPUï¼Œç›¸è¼ƒæ–¼ä¸ç©©å®šçš„colabï¼Œçµ•å°è¶³ä»¥æ»¿è¶³æ—¥å¸¸çš„å·¥ä½œä»»å‹™ã€‚
### æœ¬äººå·²è„«é›¢æ”¿æ²»å·¥ä½œï¼Œä¹Ÿä¸çŸ¥é“èª°æœƒçœ‹åˆ°æœ¬å°ˆæ¡ˆ(å› ç‚ºé€šå¸¸é€™é¡å·¥ä½œçš„äººæ ¹æœ¬ä¸æœƒæ‰“é–‹github)ï¼Œå¹«åŠ©å¾Œè¼©å°‘èµ°å½è·¯æ˜¯æˆ‘å°å‰ä¸€ä»½å·¥ä½œçš„åŸ·å¿µï¼Œç›®å‰é€™äº›ä»£ç¢¼é›†æˆå·²ç¶“æ¥è¿‘ç©©å®šï¼Œå¾ŒçºŒæ˜¯å¦é‚„æœ‰å„ªåŒ–ç©ºé–“æˆ‘å†å•å•GPT~ **[æ›´æ–°]** **æ–°ä»£ç¢¼æ•´åˆäº†æ‰¹æ¬¡è™•ç† (Batch Processing) å’Œæ›´ç²¾ç´°çš„ä¸¦è¡Œæ§åˆ¶ï¼Œæ•ˆç‡æ›´é«˜ã€‚**
### çš„ç¢ºç¶²è·¯ä¸Šæœ‰æ¯”è¼ƒå¿«çš„Demoï¼Œæ¯”å¦‚whisperJAXã€Whisper web gpuç”šè‡³groq apiç­‰ç­‰ï¼Œä½†ç„¡æ³•ä½¿ç”¨å®¢è£½åŒ–æ¨¡å‹ï¼Œä¹Ÿç„¡æ³•ä½¿ç”¨è¼ƒå¤§çš„éŸ³æª”(é€šå¸¸è¶…é25MB~=30åˆ†é˜ä½éŸ³å€¼mp3æª”å°±ä¸å¤ªèƒ½ç”¨)
### â˜†è´ˆèˆ‡æœ‰ç·£äºº~åæ­£æ•´å¥—ç›®å‰éƒ½ä¸ç”¨èŠ±éŒ¢~
### ~~â˜†æ–°å¢ï¼š**FW1.1.0 with Batched pipeline full code (2024/11/22)~~ **[æ›´æ–°]** **ç›®å‰ä»£ç¢¼å·²æ•´åˆ `BatchedInferencePipeline`ï¼Œä¸¦æä¾›æ›´å®Œå–„çš„è‡ªå‹•åŒ–è™•ç†æµç¨‹ (2025/04/22)ã€‚**

## ç›®éŒ„

- [åŠŸèƒ½](#åŠŸèƒ½)
- [æŠ€è¡“ç‰¹è‰²](#æŠ€è¡“ç‰¹è‰²)
- [å®‰è£èˆ‡è¨­å®š](#å®‰è£èˆ‡è¨­å®š)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [ç¯„ä¾‹ä»£ç¢¼](#ç¯„ä¾‹ä»£ç¢¼)
- [è²¢ç»](#è²¢ç»)
- [æˆæ¬Šæ¢æ¬¾](#æˆæ¬Šæ¢æ¬¾)
- [è‡´è¬](#è‡´è¬)
- [è¯ç¹«æ–¹å¼](#è¯ç¹«æ–¹å¼)

## åŠŸèƒ½

1.  **[æ›´æ–°]** **å¤š GPU è‡ªå‹•åŒ–ä¸¦è¡Œè™•ç†**
    *   **è‡ªå‹• GPU åµæ¸¬èˆ‡åˆ†é…**ï¼šè‡ªå‹•åµæ¸¬ Kaggle ç’°å¢ƒä¸­å¯ç”¨çš„ T4 GPU æ•¸é‡ (æœ€å¤šæ”¯æ´4å€‹)ï¼Œä¸¦ä»¥è¼ªè©¢ (Round-Robin) æ–¹å¼å°‡éŸ³æª”åˆ†é…çµ¦ä¸åŒ GPU è™•ç†ã€‚
    *   **æ‰¹æ¬¡æ¨ç† (Batch Inference)**ï¼šåˆ©ç”¨ `BatchedInferencePipeline` å°‡å¤šå€‹è½‰éŒ„è«‹æ±‚æ‰“åŒ…è™•ç†ï¼Œå¤§å¹…æå‡ GPU åˆ©ç”¨ç‡å’Œæ•´é«”ååé‡ã€‚
    *   **å–® GPU ä¸¦è¡Œæ§åˆ¶**ï¼šå¯è¨­å®šæ¯å¼µ GPU åŒæ™‚è™•ç†çš„æœ€å¤§ä»»å‹™æ•¸ (`MAX_CONCURRENCY_PER_GPU`)ï¼Œé¿å…å–®ä¸€ GPU éè¼‰ï¼Œç¢ºä¿ç©©å®šé‹è¡Œã€‚
    *   **è½‰éŒ„æ–‡æœ¬åˆä½µèˆ‡æ™‚é–“æˆ³è¨˜æ¥çºŒ**ï¼šå°‡å¤šå€‹è½‰éŒ„çµæœåˆä½µç‚ºä¸€å€‹æ–‡ä»¶ï¼Œä¸¦è‡ªå‹•æ¥çºŒæ™‚é–“æˆ³è¨˜ï¼Œç¢ºä¿æ™‚é–“è»¸é€£çºŒã€‚

2.  **æ¨¡å‹æœ¬åœ°åŒ–èˆ‡åŠ è¼‰å„ªåŒ–**
    *   **æ¨¡å‹é å…ˆä¸‹è¼‰èˆ‡æœ¬åœ°åŠ è¼‰**ï¼šæå‰ä¸‹è¼‰é©ç”¨ `faster-whisper` çš„æ¨¡å‹ä¸¦ä¸Šå‚³è‡³ Kaggle çš„ Datasets æˆ– Modelsï¼Œé¿å…æ¯æ¬¡é‹è¡Œæ™‚é‡æ–°ä¸‹è¼‰ï¼Œæé«˜é ç†±é€Ÿåº¦ã€‚
    *   **[æ–°å¢]** **å¤š GPU æ¨¡å‹ç¨ç«‹åŠ è¼‰**ï¼šç‚ºæ¯å€‹åµæ¸¬åˆ°çš„ GPU ç¨ç«‹åŠ è¼‰æ¨¡å‹å¯¦ä¾‹ï¼Œç¢ºä¿ä¸¦è¡Œè™•ç†é †æš¢ã€‚

3.  **è½‰éŒ„æ–‡æœ¬çš„åˆ†æ®µèˆ‡æ‰“å°å„ªåŒ–**
    *   **å›ºå®šæ™‚é–“é–“éš”åˆ†æ®µ**ï¼šåŸºæ–¼å¯é…ç½®çš„ `SEGMENT_DURATION` (é è¨­ 30 ç§’) æ™‚é–“é–“éš”é€²è¡Œæ–‡æœ¬åˆ†æ®µï¼Œæå‡ä¸Šä¸‹æ–‡å¯è®€æ€§ï¼Œæ–¹ä¾¿å¾ŒçºŒæ ¡æ­£ã€‚
    *   **å³æ™‚é€²åº¦æ‰“å°**ï¼šè½‰éŒ„éç¨‹ä¸­ï¼Œé€æ®µæ‰“å°å¸¶æ™‚é–“æˆ³çš„æ–‡æœ¬åˆ°æ§åˆ¶å°ï¼Œæ–¹ä¾¿å³æ™‚ç›£æ§é€²åº¦ã€‚**[æ–°å¢]**
    *   **é¦–æ®µèµ·é»ä¿®æ­£**ï¼šä»¥ç¬¬ä¸€å€‹èªéŸ³ç‰‡æ®µçš„ start ä½œç‚ºæ®µè½èµ·é»ï¼Œé¿å…å‰ç½®éœéŸ³é€ æˆæ™‚é–“æˆ³èª¤å·®ã€‚
    *   **geminiå¾ŒçºŒæ ¡æ­£**ï¼šåˆ©ç”¨Google AI Studioå¯èª¿ç”¨å…è²»gemini 2.5 /proflashé€²è¡Œé«˜ç²¾åº¦å…è²»æ ¡ç¨¿ï¼Œç¶“å¤šæ¬¡æ¸¬è©¦é©—è­‰ï¼Œæ¯æ¬¡æ ¡æ­£ç´„6500tokenä¹‹ä»½é‡å¯ä»¥å…¼é¡§ç©©å®šçš„æ ¡æ­£å“è³ªä»¥åŠæ•ˆç‡ã€‚

4.  **[æ›´æ–°]** **æ˜“ç”¨æ€§èˆ‡éŒ¯èª¤è™•ç†**
    *   **è‡ªå‹•éŸ³æª”æƒæ**ï¼šè‡ªå‹•éè¿´æƒææŒ‡å®šæ ¹ç›®éŒ„ (`AUDIO_ROOT`) ä¸‹çš„æ‰€æœ‰éŸ³æª” (æ”¯æ´ `.wav`, `.flac`, `.mp3`, `.ogg` ç­‰æ ¼å¼)ï¼Œç„¡éœ€æ‰‹å‹•æŒ‡å®šæ¯å€‹æ–‡ä»¶ã€‚
    *   **è‡ªå‹•è¼¸å‡ºå‘½å**ï¼šæ ¹æ“šæƒæåˆ°çš„éŸ³æª”é †åºï¼Œè‡ªå‹•ç”Ÿæˆ `01.txt`, `02.txt`... ç­‰è¼¸å‡ºæª”åã€‚
    *   **åƒæ•¸é›†ä¸­ç®¡ç†**ï¼šå°‡å¸¸ç”¨åƒæ•¸ (æ¨¡å‹è·¯å¾‘ã€éŸ³æª”æ ¹ç›®éŒ„ã€æ‰¹æ¬¡å¤§å°ã€ä¸¦è¡Œæ•¸ã€æ›¿æ›è©ç­‰) é›†ä¸­åœ¨ä»£ç¢¼é–‹é ­ï¼Œæ–¹ä¾¿ä¿®æ”¹ã€‚
    *   **éŒ¯èª¤æ•æ‰**ï¼šå°å–®å€‹æ–‡ä»¶çš„è½‰éŒ„éç¨‹é€²è¡ŒéŒ¯èª¤æ•æ‰ï¼Œé¿å…å–®ä¸€æ–‡ä»¶å¤±æ•—å°è‡´æ•´å€‹ç¨‹åºä¸­æ–·ã€‚

5.  **æ™‚é–“æˆ³è¨˜ä½ç½®èˆ‡æ ¼å¼èª¿æ•´**
    *   **æ™‚é–“æˆ³è¨˜ç½®æ–¼æ®µè½é–‹é ­**ï¼šçµ±ä¸€æ™‚é–“æˆ³è¨˜æ ¼å¼ (`HH:MM:SS-HH:MM:SS`)ï¼Œç¢ºä¿æ¯å€‹æ®µè½çš„æ™‚é–“æˆ³è¨˜ä½æ–¼æ–‡æœ¬æœ€å‰ç«¯ã€‚

## æŠ€è¡“ç‰¹è‰²

- **[æ›´æ–°]** **é«˜æ•ˆæ‰¹æ¬¡æ¨ç†**ï¼šåˆ©ç”¨ `BatchedInferencePipeline` å¯¦ç¾é«˜æ•ˆæ‰¹æ¬¡è™•ç†ï¼Œæœ€å¤§åŒ– GPU ååé‡ã€‚
- **[æ›´æ–°]** **è‡ªå‹•åŒ–å¤š GPU ç®¡ç†**ï¼šè‡ªå‹•åµæ¸¬ä¸¦åˆ©ç”¨æ‰€æœ‰å¯ç”¨ GPU (æœ€å¤š4å€‹)ï¼Œæ™ºèƒ½åˆ†é…ä»»å‹™ã€‚
- **[æ›´æ–°]** **ç²¾ç´°åŒ–ä¸¦è¡Œæ§åˆ¶**ï¼šé€šé `threading.Semaphore` æ§åˆ¶å–®å¼µ GPU çš„ä¸¦è¡Œä»»å‹™æ•¸ï¼Œå¹³è¡¡æ•ˆç‡èˆ‡ç©©å®šæ€§ã€‚
- **[æ›´æ–°]** **è‡ªå‹•åŒ–æ–‡ä»¶è™•ç†**ï¼šè‡ªå‹•æƒæéŸ³æª”ã€è‡ªå‹•å‘½åè¼¸å‡ºæ–‡ä»¶ï¼Œç°¡åŒ–æ“ä½œæµç¨‹ã€‚
- **é«˜æ•ˆçš„å¤šåŸ·è¡Œç·’è™•ç†**ï¼šä½¿ç”¨ `concurrent.futures.ThreadPoolExecutor` å¯¦ç¾å¤šåŸ·è¡Œç·’èª¿åº¦ã€‚~~ç¶“æ¸¬è©¦ï¼Œå¤šé€²ç¨‹æ•ˆç‡ä½æ–¼å¤šç·šç¨‹(2024/12/11ï¼ŒFW=1.1.0)~~ **[è¨»]** **æ–°ä»£ç¢¼ä¾ç„¶ä½¿ç”¨å¤šç·šç¨‹ï¼Œçµåˆ Semaphore æ§åˆ¶ä¸¦è¡Œåº¦ã€‚**
- **å„ªåŒ–çš„æ¨¡å‹åŠ è¼‰**ï¼šå°‡ `faster-whisper` é©ç”¨çš„æ¨¡å‹ä¸Šå‚³ Kaggleï¼Œé¿å…æ¯æ¬¡é‡è¤‡ä¸‹è¼‰ï¼Œæ¸›å°‘è½‰éŒ„ä»»å‹™å‰çš„æº–å‚™æ™‚é–“ã€‚
- **éˆæ´»çš„æ–‡æœ¬åˆ†æ®µæ–¹å¼**ï¼šæ”¯æŒå›ºå®šæ™‚é–“é–“éš” (`SEGMENT_DURATION`) åˆ†æ®µæ–¹å¼ï¼Œæå‡è½‰éŒ„æ–‡æœ¬çš„å¯è®€æ€§ã€‚
- **è‡ªå‹•åŒ–çš„è½‰éŒ„æ–‡æœ¬åˆä½µ**ï¼šè‡ªå‹•æ¥çºŒæ™‚é–“æˆ³è¨˜ï¼Œç¢ºä¿å¤šå€‹è½‰éŒ„æ–‡ä»¶çš„æ™‚é–“è»¸é€£çºŒã€‚

## å®‰è£èˆ‡è¨­å®š

### ç’°å¢ƒéœ€æ±‚
- **Python ç‰ˆæœ¬**: 3.8+
- **å¹³å°**: Kaggleï¼ˆKaggle å·²ç¶“ç‚ºä½ é å…ˆé…ç½®äº†æ‰€éœ€ç’°å¢ƒï¼‰
- **ç¡¬é«”**: å…©å¼µ T4 GPUï¼ˆç”± Kaggle æä¾›å…è²»è³‡æºï¼‰

### é‹è¡Œæ­¥é©Ÿ

#### 1. å®‰è£å¿…è¦çš„ Python å¥—ä»¶

åœ¨æ–°é–‹çš„ Kaggle Notebook ä¸­ï¼ŒåŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä¾†å®‰è£ `faster-whisper` å¥—ä»¶ï¼š(ç´„20ç§’)
#### 2024/10/26**æ›´æ–°**:ctranslate2æœ€æ–°ç‰ˆåœ¨cudaç›¸å®¹æ€§ä¸Šè²Œä¼¼å‡ºç¾å•é¡Œï¼Œç›®å‰ä»¥é€€å›ç‰ˆæœ¬æ–¹å¼è™•ç†
#### 2024/12/11**æ›´æ–°**:~FW1.1.0ç‰ˆæœ¬ä¸­å•é¡Œä¼¼ä¹å·²ç¶“è§£æ±ºï¼Œå¯ä»¥æ‹¿æ‰ctranslate2==4.4.0ã€‚~
#### 2025/04/22**æ›´æ–°**:~FW1.2.0ç‰ˆæœ¬ä¸‹å•é¡Œå—é™æ–¼å¹³å°ç’°å¢ƒçš„ä¾è³´ç‰ˆæœ¬ï¼Œç›®å‰ä»éœ€ctranslate2==4.4.0ã€‚ **[è¨»]**:å¹³å°ç’°å¢ƒç›®å‰ä¸ç©©å®šï¼Œå»ºè­°å›æ»¾åˆ°å»å¹´ä»¥å‰çš„ç’°å¢ƒæˆ–ç­‰æ›´æ–°~
#### 2025/06/30**æ›´æ–°**:`ctranslate2` å·²å‡ç´šï¼Œä¸¦éš¨ `faster-whisper==1.1.1` ä¸€ä½µå®‰è£ï¼Œç›¸é—œ CUDA ç›¸å®¹æ€§å•é¡Œå·²è§£æ±ºï¼Œç„¡éœ€å†é¡å¤–å®‰è£ `ctranslate2==4.4.0`ã€‚
#### 2025/08/11**æ›´æ–°**:æ›´æ–°ç‰ˆæœ¬faster-whisper==1.2.0

```python
# å®‰è£ faster-whisper (å·²å…§å« ctranslate2)
!pip install faster-whisper==1.2.0 -q
```

#### 2. ä¸Šå‚³æ¨¡å‹è‡³ Kaggleï¼Œä¸¦åœ¨notebookä¸­åŠ è¼‰

æ¥ä¸‹ä¾†ï¼Œå°‡ `faster-whisper` æ¨¡å‹å¾ Hugging Face ä¸‹è¼‰ï¼Œä¸¦ä¸Šå‚³è‡³ Kaggleã€‚è«‹éµå¾ªä»¥ä¸‹æ­¥é©Ÿï¼š

1. å‰å¾€ Hugging Face æ¨¡å‹é é¢ï¼šç¯„ä¾‹[faster-whisper-large-v2-zh-TW](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW)
2. é»æ“Šé é¢ä¸­çš„ **Files and version** æŒ‰éˆ•ï¼Œå°‡æ¨¡å‹çš„æ‰€æœ‰æª”æ¡ˆä¸‹è¼‰åˆ°æœ¬åœ°é›»è…¦ã€‚
3. ç™»å…¥ Kaggleï¼Œä¸¦åœ¨é é¢ä¸­é»æ“Š**creat**ï¼Œé¸æ“‡ **Create New model**ã€‚
4. åœ¨å»ºç«‹æ–°çš„ model æ™‚ï¼Œç‚ºå®ƒå–ä¸€å€‹åç¨±ï¼Œä¾‹å¦‚ï¼š`faster-whisper-large-v2-zh-TW`ã€‚
5. å°‡ä¸‹è¼‰å¥½çš„æ‰€æœ‰æª”æ¡ˆä¸Šå‚³åˆ°è©² model è³‡æ–™å¤¾ä¸­ã€‚
6. ä¸Šå‚³å®Œæˆå¾Œï¼Œé»æ“Š **Create** æŒ‰éˆ•ï¼Œå³å¯å®Œæˆæ¨¡å‹ä¸Šå‚³è‡³ Kaggleã€‚
7. é¦–æ¬¡ä½¿ç”¨é …ç›®æ™‚ï¼Œé»æ“Šå³é‚Š"input"ä¸‹çš„"add input"ï¼Œé¸"models"ä»¥åŠ"your work"æ­£ç¢ºæ‰¾åˆ°ä½ ä¸Šå‚³å¥½çš„modelï¼Œä¹‹å¾Œå†å•Ÿå‹•notebokkå°±æœƒè‡ªå‹•åŠ è¼‰(æ‹æ‰‹~)

å‚™è¨»ï¼šlargeV3æ¨¡å‹å¾®èª¿çš„modelåœ¨è½‰éŒ„é€Ÿåº¦ä¸Šæœƒæ¯”largeV2çš„å¿«10%~20%ï¼Œä½†ç²¾åº¦ä¸Šï¼ŒåŸºæ–¼largeV2å¾®èª¿çš„æ¨¡å‹å°æ–¼ä¸­æ–‡çš„é©æ‡‰æ€§è¼ƒä½³ï¼Œç¯„ä¾‹æä¾›çš„ç‰ˆæœ¬å·²ç¶“æ˜¯ç²¾åº¦æœ€ä½³çš„ç‰ˆæœ¬ä¹‹ä¸€ã€‚

####  [æ›´æ–°] ä¸Šå‚³éŸ³æª”è‡³ Kaggle Dataset


1. å¾ä½ çš„è£ç½®å°‡æ¬²è½‰éŒ„çš„éŸ³æª”åˆ‡ä¸€åŠï¼Œä½ å¯ä»¥ç”¨ffmpegæˆ–æ˜¯ç”¨å‰ªæ˜ éƒ½å¯
2. è·Ÿä¸Šå‚³æ¨¡å‹æ­¥é©Ÿé¡ä¼¼ï¼Œåªæ˜¯è¦é¸æ“‡datasetï¼Œä½ å¯ä»¥åœ¨åŒä¸€å€‹datasetä¸­ä¸Šå‚³è¤‡æ•¸å€‹éŸ³æª”
3. ä½¿ç”¨notebookçš„æ™‚å€™"add input"å°‡éŸ³æª”åŒ¯å…¥notebookä¸­ï¼Œæ–¹æ³•è·Ÿmodelä¸€æ¨£
4. ä¹‹å¾Œä»£ç¢¼å°è‡ªå‹•æ•æ‰éŸ³æª”ï¼Œä¸ç”¨ç®¡ä»–


## ä½¿ç”¨æ–¹æ³•

åœ¨æ–°é–‹çš„kaggle notebookä¸­ï¼Œæ ¹æ“šä»¥ä¸‹ä»£ç¢¼é€æ­¥åŸ·è¡Œï¼Œç­‰æ–¼å‚»ç“œæ“ä½œå–”~

## ç¯„ä¾‹ä»£ç¢¼
(è«‹æŒ‰ä»£ç¢¼å¡Šè¤‡è£½è²¼ä¸Šå°±è¡Œï¼Œè®“ä½ æ”¹ä½ å†æ”¹)

### è½‰éŒ„éŸ³é »æ–‡ä»¶
1. æ³¨æ„MODEL_PATH = "è¤‡è£½ä½ inputçš„whisper modelçš„è·¯å¾‘(ç›´æ¥é»COPYå°±å¥½)"
2. å…¶ä»–åƒæ•¸å·²ç¶“ç¶“éè¶…é300å°æ™‚ä»¥ä¸Šçš„è½‰éŒ„ä»»å‹™å¯¦éš›é©—è­‰ï¼Œä¸å¤ªéœ€è¦èª¿æ•´
3. replacements(ç”¨Ctrl+fæŸ¥æ‰¾)éƒ¨åˆ†èƒ½æä¾›å›ºå®šè½‰éŒ„ä»»å‹™(æ¯”å¦‚å¤šæ¬¡è½‰éŒ„åŒä¸€å€‹è¬›è€…çš„éŸ³æª”)è¼ƒä½³çš„é«”é©—ï¼ŒæŠŠå›ºå®šçš„éŒ¯æ¼å­—ç›´æ¥æ›¿æ›æˆæ­£ç¢ºçš„æ–‡å­—ï¼Œæ ¼å¼ç‚º"éŒ¯å­—": "æ­£ç¢ºå­—",
4. æœ¬å°ˆæ¡ˆ(æŒ‰ç¯„ä¾‹æ¨¡å‹)å¯¦æ¸¬3å°æ™‚éŸ³é »æ–‡ä»¶(WAVæª”ï¼Œ256KåŒå¸¸å¤§å°ç´„300MB)çš„è½‰éŒ„ä»»å‹™ç´„éœ€3è‡³4é˜è½‰éŒ„æ™‚é–“ï¼Œæº–ç¢ºåº¦å¹³å‡åœ¨90%ä»¥ä¸Šï¼Œå†é€égeminiæ ¡æ­£ï¼Œæº–ç¢ºç‡å¯é”99%ï¼Œä¸å”¬çˆ›ã€‚
5. éŒ„éŸ³ç­†å»ºè­°é è¨­éŒ„è£½WAV/flacæª”ï¼Œç²¾åº¦ç¢ºå¯¦å„ªæ–¼MP3æª”ã€‚WAVæª”(192K&256K)å¤§æ¦‚æ˜¯éŸ³è³ªå½±éŸ¿ç²¾åº¦çš„æ¥µé™ï¼Œå†å¤§å‰‡ç„¡ç”¨ã€‚

```python
%%time
from faster_whisper import WhisperModel, BatchedInferencePipeline
import datetime, time, os, re, torch, glob
from typing import List, Tuple, Dict
import concurrent.futures, threading, multiprocessing as mp

%%time
# ================================================================
# 1. åƒæ•¸å€ï¼ˆå¯èª¿ï¼‰
# ================================================================
# [1.0] è·¯å¾‘è¨­å®š
MODEL_PATH = "/kaggle/working/1"
AUDIO_ROOT = "/kaggle/input"            # æ›´æ›è³‡æ–™å¤¾åªæ”¹é€™è£¡
AUDIO_EXTS = (".wav", ".flac", ".mp3", ".ogg")

# [1.2] è½‰éŒ„èˆ‡åˆ†æ®µ
SEGMENT_DURATION = 30.0                 # ä¸€è¡Œæœ€å¤šè¦†è“‹çš„ã€Œç‰†é˜æ™‚é–“ã€ç§’æ•¸
BATCH_SIZE = 24
MAX_CONCURRENCY_PER_GPU = 2

# [1.3] æ–‡æœ¬æ¸…ç†èˆ‡åˆå§‹æç¤º
REPLACEMENTS: Dict[str, str] = {"èª²é¡Œ": "å®¢é«”", "ç·ç½ª": "æ—¢é‚"}
INITIAL_PROMPT = "æ³•å¾‹"

# ================================================================
# 2. å…¬ç”¨å°å·¥å…·
# ================================================================
# [2.1] æ”¶é›†éŸ³æª”æ¸…å–®
def collect_audio_files(root: str, exts=AUDIO_EXTS) -> List[str]:
    exts_lower = {e.lower() for e in exts}
    files = []
    for dirpath, _, filenames in os.walk(root):
        for fn in filenames:
            ext = os.path.splitext(fn)[1].lower()
            if ext in exts_lower:
                files.append(os.path.join(dirpath, fn))
    return sorted(files)

# [2.2] ç§’ â†’ æ™‚:åˆ†:ç§’
def to_timestamp(sec: float) -> str:
    h = int(sec // 3600)
    m = int((sec % 3600) // 60)
    s = int(sec % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"

# [2.3] æ ¼å¼åŒ–ä¸€è¡Œè¼¸å‡º
def fmt_chunk(start: float, end: float, txt: str) -> str:
    return f"{to_timestamp(start)}-{to_timestamp(end)} {txt.strip()}\n"

# [2.4] æ–‡æœ¬æ¸…ç†ï¼ˆé—œéµè©æ›¿æ› + å»å‰ç¶´ç¬¦è™Ÿï¼‰
_pattern = re.compile("|".join(re.escape(k) for k in REPLACEMENTS)) if REPLACEMENTS else None
def clean_text(txt: str) -> str:
    txt = txt.lstrip("! ").strip()
    if _pattern:
        txt = _pattern.sub(lambda m: REPLACEMENTS[m.group(0)], txt)
    return txt

# [2.5] æ®µè½è¼¸å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰
def process_segments(segments, outfile: str, max_len=SEGMENT_DURATION):
    """
    [2.5] æ®µè½è¼¸å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰
    ç›®çš„ï¼šä¿®æ­£é¦–æ®µèµ·é»ç¡¬å¾ 0:00 ç®—çš„å•é¡Œï¼›æ”¹æˆã€Œä»¥ç¬¬ä¸€å€‹ seg.start ç‚ºèµ·é»ã€ã€‚
         é€™æ¨£ä¸æœƒåœ¨éŸ³æª”å‰ç½®éœéŸ³æ™‚ï¼ŒæŠŠæ™‚é–“å€é–“ç®—å¾—éé•·ã€‚
    è¦å‰‡ï¼š
      (1) ä¸€è¡Œæœ€å¤šæ¶µè“‹ max_len ç§’ï¼ˆç”¨ç‰†é˜æ™‚é–“ï¼šæœ€å¾Œä¸€å€‹ seg.end - è©²è¡Œç¬¬ä¸€å€‹ seg.startï¼‰ã€‚
      (2) æ›è¡Œå¾Œï¼Œä¸‹ä¸€è¡Œçš„èµ·é» = ä¸‹ä¸€å€‹å¯¦éš› seg.startï¼ˆä¸å£“ç¸®ä¸­é–“éœéŸ³ï¼‰ã€‚
    è¨»ï¼šè‹¥ä½ åå¥½ã€Œå£“ç¸®éœéŸ³ã€ï¼ˆä¸‹ä¸€è¡Œèµ·é»æ”¹æ¥å‰ä¸€è¡Œçš„ last_endï¼‰ï¼Œ
        å¯æŠŠ `chunk_start = None` çš„é‚è¼¯ï¼Œæ”¹ç‚º `chunk_start = last_end`ã€‚
    """
    buf_lines = []
    chunk_start = None
    chunk_text_parts = []
    last_end = None

    for seg in segments:
        if chunk_start is None:
            chunk_start = float(getattr(seg, "start", 0.0))
            chunk_text_parts = []

        chunk_text_parts.append(clean_text(seg.text))
        last_end = float(getattr(seg, "end", chunk_start))

        if (last_end - chunk_start) >= max_len:
            line_txt = " ".join(chunk_text_parts).strip()
            line = fmt_chunk(chunk_start, last_end, line_txt)
            print(line, end="", flush=True)
            buf_lines.append(line)

            chunk_start = None
            chunk_text_parts = []
            last_end = None

    if chunk_text_parts:
        end_time = last_end if last_end is not None else chunk_start
        line_txt = " ".join(chunk_text_parts).strip()
        line = fmt_chunk(chunk_start, end_time, line_txt)
        print(line, end="", flush=True)
        buf_lines.append(line)

    with open(outfile, "w", encoding="utf-8") as fh:
        fh.write("".join(buf_lines))
    print(f" âœ” å·²å¯«å…¥ {outfile}")

# ================================================================
# 3. å­é€²ç¨‹åŸ·è¡Œé‚è¼¯ï¼ˆæ¯å¼µ GPU ä¸€æ”¯ï¼‰
# ================================================================
def gpu_worker(gpu_idx: int,
               jobs: List[Tuple[str, str]],
               semaphore_size: int,
               result_queue: mp.Queue):
    """
    [3.1] å­é€²ç¨‹åªçœ‹å¾—è¦‹è‡ªå·±çš„ GPUï¼ˆé€é CUDA_VISIBLE_DEVICESï¼‰
    [3.2] è¼‰å…¥ WhisperModel â†’ å»º Pipelineï¼ˆåš´ç¦åœ¨çˆ¶é€²ç¨‹å…ˆåˆå§‹åŒ– CUDAï¼‰
    [3.3] ç”¨ ThreadPoolExecutor + Semaphore æ§å–®å¡ä½µç™¼
    [3.4] æŠŠæ¯å€‹ job çš„çµæœï¼ˆæˆåŠŸ/å¤±æ•—ï¼‰å¯«é€² multiprocessing.Queue
    """
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_idx)
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS", "1")
    try:
        torch.set_num_threads(1)
    except Exception:
        pass

    dev = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[GPU{gpu_idx}] é–‹å§‹åˆå§‹åŒ–æ¨¡å‹ ...")
    tic = time.time()

    model = WhisperModel(MODEL_PATH,
                         device=dev,
                         device_index=0,
                         compute_type="float16")
    pipeline = BatchedInferencePipeline(model=model)
    print(f"[GPU{gpu_idx}] æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œç”¨æ™‚ {time.time()-tic:.1f}s")

    semaphore = threading.Semaphore(semaphore_size)

    def transcribe_one(in_path: str, out_path: str):
        with semaphore:
            try:
                segments, _ = pipeline.transcribe(
                    in_path,
                    batch_size=BATCH_SIZE,
                    word_timestamps=True,
                    hallucination_silence_threshold=3,
                    initial_prompt=INITIAL_PROMPT or None,
                    beam_size=5,
                    temperature=0,
                    patience=1.5,
                    language="zh",
                    max_new_tokens=256,
                    condition_on_previous_text=False,
                    vad_filter=True,
                    vad_parameters={
                        "min_silence_duration_ms": 250,
                        "speech_pad_ms": 600
                    },
                    log_progress=True,
                )
                process_segments(segments, out_path)
                result_queue.put((in_path, "âœ“"))
            except Exception as exc:
                print(f"[GPU{gpu_idx}] âœ˜ è½‰éŒ„å¤±æ•—: {in_path} ({exc})")
                result_queue.put((in_path, "âœ˜"))

    with concurrent.futures.ThreadPoolExecutor(max_workers=semaphore_size) as pool:
        pool.map(lambda pair: transcribe_one(*pair), jobs)

    result_queue.put(("__DONE__", gpu_idx))

# ================================================================
# 4. ä¸»æµç¨‹
# ================================================================
def main():
    audio_files = collect_audio_files(AUDIO_ROOT)
    if not audio_files:
        raise RuntimeError(f"æ‰¾ä¸åˆ°ä»»ä½•éŸ³æª”æ–¼ {AUDIO_ROOT}")
    print(f"å…±æ‰¾åˆ° {len(audio_files)} å€‹éŸ³æª”")

    gpu_count = min(torch.cuda.device_count() or 1, 4)
    print(f"åµæ¸¬åˆ° GPU æ•¸é‡ï¼š{gpu_count}")
    jobs_by_gpu: Dict[int, List[Tuple[str, str]]] = {i: [] for i in range(gpu_count)}
    for idx, path in enumerate(audio_files):
        base = os.path.splitext(os.path.basename(path))[0]
        jobs_by_gpu[idx % gpu_count].append((path, f"{base}.txt"))

    ctx = mp.get_context("fork")
    result_queue = ctx.Queue()
    processes = []
    for gpu_idx in range(gpu_count):
        p = ctx.Process(target=gpu_worker,
                        args=(gpu_idx,
                              jobs_by_gpu[gpu_idx],
                              MAX_CONCURRENCY_PER_GPU,
                              result_queue))
        p.start()
        processes.append(p)

    finished_gpu = set()
    while len(finished_gpu) < gpu_count:
        item = result_queue.get()
        if item[0] == "__DONE__":
            finished_gpu.add(item[1])
            continue
        in_path, status = item
        print(f"[ä¸»æ§] {status} {in_path}")

    for p in processes:
        p.join()
    print("ğŸ‰ æ‰€æœ‰è½‰éŒ„ä»»å‹™å®Œæˆï¼")

# ================================================================
# 5. Entry point
# ================================================================
if __name__ == "__main__":
    t0 = time.time()
    main()
    print(f"ç¸½è€—æ™‚ï¼š{time.time() - t0:.1f} ç§’")
```

### åˆä½µè½‰éŒ„æ–‡æœ¬
æ‰¿ä¸Šä»£ç¢¼ï¼Œé å‘½åè¦å‰‡ç‚º01.txt->02.txtï¼Œå¦‚éœ€åˆä½µè«‹ä½¿ç”¨ä»¥ä¸‹ä»£ç¢¼ï¼Œç¢ºå®šæœ€åº•ä¸‹çš„çµ„åˆæ˜¯å¦æ­£ç¢ºï¼Œå³å¯æŒ‰éœ€åˆä½µå›‰~(æª”åé è¨­ç‚ºmerged_output.txt)
```python
def merge_transcriptions(file1, file2, output_file):
    def parse_timestamp(timestamp_str):
        """å°‡ XX:XX:XX æ ¼å¼çš„æ™‚é–“æˆ³è¨˜è½‰æ›ç‚ºç§’æ•¸"""
        h, m, s = map(int, timestamp_str.split(':'))
        return h * 3600 + m * 60 + s

    def format_timestamp(seconds):
        """å°‡ç§’æ•¸è½‰æ›ç‚º XX:XX:XX æ ¼å¼çš„æ™‚é–“æˆ³è¨˜"""
        return format_to_custom_timestamp(seconds)
    
    merged_content = ""
    total_duration = 0

    # è®€å–ç¬¬ä¸€å€‹æ–‡ä»¶çš„å…§å®¹
    with open(file1, 'r', encoding="utf-8") as f1:
        for line in f1:
            # å‡è¨­æ™‚é–“æˆ³è¨˜åœ¨è¡Œçš„é–‹é ­ï¼Œä¸¦ä¸”æ ¼å¼ç‚º XX:XX:XX-XX:XX:XX
            time_range, text = line.split(' ', 1)
            start_time_str, end_time_str = time_range.split('-')
            
            # å°‡æ™‚é–“æˆ³è¨˜è½‰æ›ç‚ºç§’æ•¸
            start_time = parse_timestamp(start_time_str)
            end_time = parse_timestamp(end_time_str)
            
            # æ›´æ–°æ™‚é–“æˆ³è¨˜ä»¥åŒ…æ‹¬ç´¯ç©çš„ç¸½æ™‚é–“
            new_start_time = format_timestamp(start_time + total_duration)
            new_end_time = format_timestamp(end_time + total_duration)
            
            # åˆä½µæ›´æ–°å¾Œçš„è¡Œ
            merged_content += f"{new_start_time}-{new_end_time} {text}"
        
        # æ›´æ–°ç´¯ç©çš„ç¸½æ™‚é–“
        total_duration = parse_timestamp(new_end_time)
    
    # è®€å–ç¬¬äºŒå€‹æ–‡ä»¶çš„å…§å®¹ä¸¦åˆä½µ
    with open(file2, 'r', encoding="utf-8") as f2:
        for line in f2:
            time_range, text = line.split(' ', 1)
            start_time_str, end_time_str = time_range.split('-')
            
            # å°‡æ™‚é–“æˆ³è¨˜è½‰æ›ç‚ºç§’æ•¸
            start_time = parse_timestamp(start_time_str)
            end_time = parse_timestamp(end_time_str)
            
            # æ›´æ–°æ™‚é–“æˆ³è¨˜ä»¥åŒ…æ‹¬ç´¯ç©çš„ç¸½æ™‚é–“
            new_start_time = format_timestamp(start_time + total_duration)
            new_end_time = format_timestamp(end_time + total_duration)
            
            # åˆä½µæ›´æ–°å¾Œçš„è¡Œ
            merged_content += f"{new_start_time}-{new_end_time} {text}"
    
    # å°‡åˆä½µå¾Œçš„å…§å®¹ä¿å­˜åˆ°è¼¸å‡ºæ–‡ä»¶
    with open(output_file, 'w', encoding="utf-8") as out_file:
        out_file.write(merged_content)

    print(f"Transcriptions merged and saved to {output_file}")

# å‡è¨­å·²ç¶“æˆåŠŸç”Ÿæˆäº† 01.txt å’Œ 02.txt
merge_transcriptions("01.txt", "02.txt", "merged_output.txt")
```

## è²¢ç»

æ­¡è¿ä»»ä½•å½¢å¼çš„è²¢ç»ï¼ä¸æœƒæˆ–è¦åšèª¿æ•´è‡ªå·±å•GPTå–”~æˆ‘ä¹Ÿæ˜¯æ…¢æ…¢å•å‡ºä¾†çš„~

## æˆæ¬Šæ¢æ¬¾

æœ¬å°ˆæ¡ˆæ¡ç”¨ [MIT License](LICENSE) æˆæ¬Šï¼Œè©³æƒ…è«‹åƒé–± [LICENSE](LICENSE) æ–‡ä»¶ã€‚


## ç¬¬ä¸‰æ–¹è³‡æºæˆæ¬Š

- **faster-whisper**ï¼šæœ¬å°ˆæ¡ˆä½¿ç”¨ [faster-whisper](https://github.com/SYSTRAN/faster-whisper) åº«ï¼Œè©²åº«æ¡ç”¨ MIT æˆæ¬Šã€‚
- **faster-whisper-large-v2-zh-TW æ¨¡å‹**ï¼šæœ¬å°ˆæ¡ˆç¯„ä¾‹æ¨¡å‹ä½¿ç”¨ä¾†è‡ª [Hugging Face](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW) çš„æ¨¡å‹ï¼Œè«‹éµå®ˆå…¶æˆæ¬Šæ¢æ¬¾ã€‚

## è‡´è¬

æ„Ÿè¬ä»¥ä¸‹é …ç›®å’Œè³‡æºå°æœ¬å°ˆæ¡ˆçš„æ”¯æŒï¼š

- [faster-whisper](https://github.com/SYSTRAN/faster-whisper) é–‹ç™¼åœ˜éšŠ
- [Hugging Face](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW) æä¾›çš„å„ªç§€æ¨¡å‹è³‡æº
- [Kaggle](https://www.kaggle.com/) æä¾›çš„å…è²» GPU è³‡æº

## è¯ç¹«æ–¹å¼

å¦‚æœ‰ä»»ä½•å•é¡Œæˆ–å»ºè­°ï¼Œè«‹é€šéä»¥ä¸‹æ–¹å¼è¯ç¹«ï¼š

- **é›»å­éƒµä»¶**ï¼ša0953041880@gmail.com
- **GitHub Issues**ï¼šç•¥

æ„Ÿè¬æ‚¨çš„é—œæ³¨èˆ‡æ”¯æŒï¼é†¬~ä¸‹æ¬¡è¦‹

## English

### A lifesaver for long transcriptions for media professionals, political staff, and hardworking interns.
### This project provides an efficient, user-friendly solution designed to overcome the inefficiencies of manual **audio transcription**.

### Why Not Use Colab?
1.  **GPU Limitations and Instability:** While mounting Google Drive can simplify usage, the core bottleneck for transcription tasks remains GPU availability and stability, which can be inconsistent on Colab.
2.  **Leveraging Free Resources:** This project utilizes the two free T4 GPUs provided by the Kaggle platform, significantly boosting transcription efficiency for various tasks. **[Update]** **The new code automatically detects the number of available GPUs and distributes tasks accordingly.**
3.  **Kaggle's Advantage:** Kaggle is a free machine learning platform offering 30 hours of free GPU time per week. Compared to the unpredictable nature of Colab, this is generally sufficient for routine transcription work.

### Having moved on from political work, I'm unsure who might find this project (as people in such roles rarely browse GitHub). However, driven by a desire to help successors avoid pitfalls from my previous job, I've refined this codebase to a near-stable state. I might consult GPT for further optimization possibilities~ **[Update]** **The new code integrates Batch Processing and finer-grained parallel control for higher efficiency.**

### While faster demos exist online (e.g., WhisperJAX, Whisper WebGPU, even Groq API), they often lack support for custom models or large audio files (typically struggling with files over 25MB, roughly equivalent to a 30-minute low-bitrate MP3).

### â˜† A gift to those destined to find it ~ The entire setup is currently free ~
### ~~â˜† Added: **FW1.2.0 with Batched pipeline full code (2024/11/22)~~ **[Update]** **The current code integrates `BatchedInferencePipeline` and offers a more complete automated processing workflow (2025/04/22).**

## Table of Contents

- [Features](#features)
- [Technical Highlights](#technical-highlights)
- [Installation and Setup](#installation-and-setup)
- [Usage](#usage)
- [Example Code](#example-code)
- [Contributing](#contributing)
- [License](#license)
- [Third-Party Licenses](#third-party-licenses)
- [Acknowledgements](#acknowledgements)
- [Contact](#contact)

## Features

1.  **[Update]** **Automated Multi-GPU Parallel Processing**
    *   **Automatic GPU Detection and Assignment**: Automatically detects the number of available T4 GPUs in the Kaggle environment (supports up to 4) and assigns audio files to different GPUs using a Round-Robin strategy.
    *   **Batch Inference**: Utilizes `BatchedInferencePipeline` to process multiple transcription requests in batches, significantly improving GPU utilization and overall throughput.
    *   **Per-GPU Concurrency Control**: Allows setting the maximum number of concurrent tasks per GPU (`MAX_CONCURRENCY_PER_GPU`) to prevent overloading individual GPUs and ensure stable operation.
    *   **Transcript Merging and Continuous Timestamps**: Merges results from multiple transcriptions into a single file, automatically adjusting timestamps to ensure a continuous timeline.

2.  **Model Localization and Loading Optimization**
    *   **Pre-downloading Models and Local Loading**: Download `faster-whisper` compatible models beforehand and upload them to Kaggle Datasets or Models. This avoids re-downloading during each run, speeding up initialization.
    *   **[New]** **Independent Model Loading per GPU**: Loads a separate model instance for each detected GPU, ensuring smooth parallel processing.

3.  **Transcript Segmentation and Printing Optimization**
    *   **Fixed Time Interval Segmentation**: Segments the transcript based on a configurable `SEGMENT_DURATION` (default: 30 seconds) interval, enhancing context readability and facilitating subsequent proofreading.
    *   **Real-time Progress Printing**: Prints timestamped text segments to the console during transcription for real-time progress monitoring. **[New]**
    *   **First Segment Start Fix**: The first line now begins at the first audio segment's start time, avoiding inflated timestamps when there is leading silence.
    *   **Post-correction with Gemini**: Leverage Google AI Studio's free Gemini 1.5 Flash/Pro models for high-accuracy proofreading. Multiple tests have verified that correcting chunks of approximately 6500 tokens balances stable quality and efficiency.

4.  **[Update]** **Usability and Error Handling**
    *   **Automatic Audio File Scanning**: Automatically scans recursively for all audio files (supports `.wav`, `.flac`, `.mp3`, `.ogg`, etc.) within the specified root directory (`AUDIO_ROOT`), eliminating the need to manually specify each file.
    *   **Automatic Output Naming**: Automatically generates output filenames like `01.txt`, `02.txt`, etc., based on the order of scanned audio files.
    *   **Centralized Parameter Management**: Consolidates frequently used parameters (model path, audio root directory, batch size, concurrency level, replacements, etc.) at the beginning of the code for easy modification.
    *   **Error Handling for Individual Files**: Implements error catching for the transcription process of single files, preventing the entire program from crashing due to one failed file.

5.  **Timestamp Position and Format Adjustment**
    *   **Timestamp at the Beginning of Segment**: Standardizes the timestamp format (`HH:MM:SS-HH:MM:SS`) and places it at the very beginning of each text segment.

## Technical Highlights

- **[Update]** **Efficient Batch Inference**: Leverages `BatchedInferencePipeline` for high-throughput batch processing, maximizing GPU utilization.
- **[Update]** **Automated Multi-GPU Management**: Automatically detects and utilizes all available GPUs (up to 4), intelligently distributing tasks.
- **[Update]** **Fine-grained Concurrency Control**: Uses `threading.Semaphore` to manage the number of concurrent tasks per GPU, balancing efficiency and stability.
- **[Update]** **Automated File Handling**: Automatically scans audio files and names output files, simplifying the workflow.
- **Efficient Multi-threading**: Employs `concurrent.futures.ThreadPoolExecutor` for effective multi-threaded scheduling. ~~Testing showed multi-processing was less efficient than multi-threading (2024/12/11, FW=1.1.0).~~ **[Note]** **The new code still uses multi-threading, combined with Semaphores for concurrency control.**
- **Optimized Model Loading**: Uploading `faster-whisper` compatible models to Kaggle avoids repeated downloads, reducing the preparation time before transcription tasks.
- **Flexible Text Segmentation**: Supports segmentation by a fixed time interval (`SEGMENT_DURATION`), improving the readability of the transcribed text.
- **Automated Transcript Merging**: Automatically adjusts and continues timestamps, ensuring a coherent timeline across merged transcription files.

## Installation and Setup

### Environment Requirements
- **Python Version**: 3.8+
- **Platform**: Kaggle (Kaggle typically pre-configures the necessary environment)
- **Hardware**: Two T4 GPUs (Provided for free by Kaggle)

### Running Steps

#### 1. Install Necessary Python Packages

In a new Kaggle Notebook, execute the following command to install the `faster-whisper` package (takes about 20 seconds):
#### 2024/10/26 **Update**: The latest version of `ctranslate2` seems to have CUDA compatibility issues. Currently handled by reverting to an older version.
#### 2024/12/11 **Update**: ~~Issue seems resolved in FW1.1.0, `ctranslate2==4.4.0` might be removable.~~
#### 2025/04/22 **Update**: ~~Under FW1.2.0, due to platform environment dependency versions, `ctranslate2==4.4.0` is still required. **[Note]**: The platform environment is currently unstable; reverting to an environment from last year or waiting for updates is recommended.~~
#### 2025/06/30 **Update**: `ctranslate2` has been upgraded and is bundled with `faster-whisper==1.2.0`. The previous compatibility issue has been resolved, so you no longer need to install `ctranslate2==4.4.0` separately.
#### 2025/08/11 **Update**: faster-whisper version updated(1.2.0)

```python
# Install faster-whisper (includes ctranslate2)
!pip install faster-whisper==1.2.0 -q
```

#### 2. Upload the Model to Kaggle and Load it in the Notebook

Next, download the `faster-whisper` model from Hugging Face and upload it to Kaggle. Follow these steps:

1.  Go to the Hugging Face model page, for example: [faster-whisper-large-v2-zh-TW](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW)
2.  Click the **Files and versions** button on the page and download all model files to your local computer.
3.  Log in to Kaggle, click **Create** in the header, and select **Create New Model**.
4.  When creating the new model, give it a name, e.g., `faster-whisper-large-v2-zh-TW`.
5.  Upload all the downloaded files into this model's directory.
6.  Once uploaded, click the **Create** button to finish uploading the model to Kaggle.
7.  When using the project for the first time, click "Add input" under the "Input" section on the right panel. Select "Models" and then "Your Work" to find the model you just uploaded. Subsequently, starting the notebook will automatically load it (Applause!).

Note: Models fine-tuned from large-v3 might offer 10-20% faster transcription speeds than large-v2 based models. However, models fine-tuned from large-v2 generally adapt better to Chinese. The example model provided is one of the best in terms of accuracy.

#### 3. [Update] Upload Audio Files to Kaggle Dataset

1.  Optionally, split your audio files into manageable parts using tools like ffmpeg or CapCut (å‰ªæ˜ ). This is useful if you have very long recordings, although the current script handles full files well. The original text suggested splitting in half, likely for the older 2-GPU manual setup, but with auto-distribution, this might be less critical unless files are extremely large or you want finer control.
2.  Similar to uploading the model, create a new Kaggle **Dataset**. You can upload multiple audio files into the same dataset.
3.  When using the notebook, click "Add input" and import the dataset containing your audio files, just like you did for the model.
4.  The code will automatically detect the audio files within the specified input path; no manual path specification per file is needed.

## Usage

In a new Kaggle notebook, execute the following code blocks step-by-step. It's designed to be straightforward!

## Example Code
(Just copy and paste the code blocks. Only modify where indicated.)

### Transcribe Audio Files
1.  **Important:** Set `MODEL_PATH = "copy the path of your input whisper model here (just click COPY path)"`.
2.  Other parameters have been validated through over 300 hours of actual transcription tasks and generally don't need adjustment.
3.  The `replacements` dictionary (find using Ctrl+F) is useful for consistent tasks (e.g., transcribing the same speaker multiple times) by automatically correcting common misrecognitions. Format is `"incorrect word": "correct word",`.
4.  Based on internal testing (using the example model), transcribing a 3-hour audio file (WAV format, 256K typically around 300MB) takes approximately 3 to 4 minutes, with an average accuracy above 90%. Post-correction with Gemini can increase accuracy to 99% â€“ no exaggeration.
5.  It's recommended to record audio in WAV or FLAC format by default, as their precision is indeed superior to MP3. WAV files at 192kbps or 256kbps seem to hit the sweet spot for quality influencing accuracy; higher bitrates yield diminishing returns.

```python
%%time
from faster_whisper import WhisperModel, BatchedInferencePipeline
import datetime, time, os, re, torch, glob
from typing import List, Tuple, Dict
import concurrent.futures, threading, multiprocessing as mp

%%time
# ================================================================
# 1. Adjustable Parameters
# ================================================================
MODEL_PATH = "/kaggle/working/1"
AUDIO_ROOT = "/kaggle/input"            # Change here to switch audio source
AUDIO_EXTS = (".wav", ".flac", ".mp3", ".ogg")

# [1.2] Transcription & segmentation
SEGMENT_DURATION = 30.0                 # Max wall-clock seconds per line
BATCH_SIZE = 24
MAX_CONCURRENCY_PER_GPU = 2

# [1.3] Text cleanup & initial prompt
REPLACEMENTS: Dict[str, str] = {"èª²é¡Œ": "å®¢é«”", "ç·ç½ª": "æ—¢é‚"}
INITIAL_PROMPT = "æ³•å¾‹"

# ================================================================
# 2. Utility Functions
# ================================================================
# [2.1] Gather audio file list

def collect_audio_files(root: str, exts=AUDIO_EXTS) -> List[str]:
    exts_lower = {e.lower() for e in exts}
    files = []
    for dirpath, _, filenames in os.walk(root):
        for fn in filenames:
            ext = os.path.splitext(fn)[1].lower()
            if ext in exts_lower:
                files.append(os.path.join(dirpath, fn))
    return sorted(files)

# [2.2] Seconds -> HH:MM:SS

def to_timestamp(sec: float) -> str:
    h = int(sec // 3600)
    m = int((sec % 3600) // 60)
    s = int(sec % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"

# [2.3] Format a line

def fmt_chunk(start: float, end: float, txt: str) -> str:
    return f"{to_timestamp(start)}-{to_timestamp(end)} {txt.strip()}\n"

# [2.4] Text cleanup (keyword replacement + trim prefixes)

_pattern = re.compile("|".join(re.escape(k) for k in REPLACEMENTS)) if REPLACEMENTS else None

def clean_text(txt: str) -> str:
    txt = txt.lstrip("! ").strip()
    if _pattern:
        txt = _pattern.sub(lambda m: REPLACEMENTS[m.group(0)], txt)
    return txt

# [2.5] Segment output (revised)

def process_segments(segments, outfile: str, max_len=SEGMENT_DURATION):
    """
    [2.5] Segment output (revised)
    Purpose: fix the issue of always starting at 0:00 by using the first seg.start instead.
             This avoids inflated timestamps when there is leading silence.
    Rules:
      (1) Each line covers at most max_len seconds of wall-clock time (last seg.end - first seg.start).
      (2) After a line break, the next line starts at the next actual seg.start (no compression of silence).
    Note: If you prefer compressing silence (next line start = last_end),
          change `chunk_start = None` logic to `chunk_start = last_end`.
    """
    buf_lines = []
    chunk_start = None
    chunk_text_parts = []
    last_end = None

    for seg in segments:
        if chunk_start is None:
            chunk_start = float(getattr(seg, "start", 0.0))
            chunk_text_parts = []

        chunk_text_parts.append(clean_text(seg.text))
        last_end = float(getattr(seg, "end", chunk_start))

        if (last_end - chunk_start) >= max_len:
            line_txt = " ".join(chunk_text_parts).strip()
            line = fmt_chunk(chunk_start, last_end, line_txt)
            print(line, end="", flush=True)
            buf_lines.append(line)

            chunk_start = None
            chunk_text_parts = []
            last_end = None

    if chunk_text_parts:
        end_time = last_end if last_end is not None else chunk_start
        line_txt = " ".join(chunk_text_parts).strip()
        line = fmt_chunk(chunk_start, end_time, line_txt)
        print(line, end="", flush=True)
        buf_lines.append(line)

    with open(outfile, "w", encoding="utf-8") as fh:
        fh.write("".join(buf_lines))
    print(f" âœ” Wrote {outfile}")

# ================================================================
# 3. Worker process (one per GPU)
# ================================================================

def gpu_worker(gpu_idx: int,
               jobs: List[Tuple[str, str]],
               semaphore_size: int,
               result_queue: mp.Queue):
    """
    [3.1] Each subprocess sees only its GPU via CUDA_VISIBLE_DEVICES.
    [3.2] Load WhisperModel and build pipeline inside subprocess (avoid CUDA init in parent).
    [3.3] Use ThreadPoolExecutor + Semaphore to control concurrency on a single GPU.
    [3.4] Put each job result (success/fail) into multiprocessing.Queue.
    """
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_idx)
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS", "1")
    try:
        torch.set_num_threads(1)
    except Exception:
        pass

    dev = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[GPU{gpu_idx}] Initializing model ...")
    tic = time.time()

    model = WhisperModel(MODEL_PATH,
                         device=dev,
                         device_index=0,     # subprocess sees a single GPU
                         compute_type="float16")
    pipeline = BatchedInferencePipeline(model=model)
    print(f"[GPU{gpu_idx}] Model ready in {time.time()-tic:.1f}s")

    semaphore = threading.Semaphore(semaphore_size)

    def transcribe_one(in_path: str, out_path: str):
        with semaphore:
            try:
                segments, _ = pipeline.transcribe(
                    in_path,
                    batch_size=BATCH_SIZE,
                    word_timestamps=True,
                    hallucination_silence_threshold=3,
                    initial_prompt=INITIAL_PROMPT or None,
                    beam_size=5,
                    temperature=0,
                    patience=1.5,
                    language="zh",
                    max_new_tokens=256,
                    condition_on_previous_text=False,
                    vad_filter=True,
                    vad_parameters={
                        "min_silence_duration_ms": 250,
                        "speech_pad_ms": 600
                    },
                    log_progress=True,
                )
                process_segments(segments, out_path)
                result_queue.put((in_path, "âœ“"))
            except Exception as exc:
                print(f"[GPU{gpu_idx}] âœ˜ Transcription failed: {in_path} ({exc})")
                result_queue.put((in_path, "âœ˜"))

    with concurrent.futures.ThreadPoolExecutor(max_workers=semaphore_size) as pool:
        pool.map(lambda pair: transcribe_one(*pair), jobs)

    result_queue.put(("__DONE__", gpu_idx))

# ================================================================
# 4. Main flow
# ================================================================

def main():
    audio_files = collect_audio_files(AUDIO_ROOT)
    if not audio_files:
        raise RuntimeError(f"No audio files found in {AUDIO_ROOT}")
    print(f"Found {len(audio_files)} audio files")

    gpu_count = min(torch.cuda.device_count() or 1, 4)  # Kaggle usually <=2
    print(f"Detected GPU count: {gpu_count}")
    jobs_by_gpu: Dict[int, List[Tuple[str, str]]] = {i: [] for i in range(gpu_count)}
    for idx, path in enumerate(audio_files):
        base = os.path.splitext(os.path.basename(path))[0]
        jobs_by_gpu[idx % gpu_count].append((path, f"{base}.txt"))

    ctx = mp.get_context("fork")
    result_queue = ctx.Queue()
    processes = []
    for gpu_idx in range(gpu_count):
        p = ctx.Process(target=gpu_worker,
                        args=(gpu_idx,
                              jobs_by_gpu[gpu_idx],
                              MAX_CONCURRENCY_PER_GPU,
                              result_queue))
        p.start()
        processes.append(p)

    finished_gpu = set()
    while len(finished_gpu) < gpu_count:
        item = result_queue.get()
        if item[0] == "__DONE__":
            finished_gpu.add(item[1])
            continue
        in_path, status = item
        print(f"[Main] {status} {in_path}")

    for p in processes:
        p.join()
    print("ğŸ‰ All transcriptions completed!")

# ================================================================
# 5. Entry point
# ================================================================

if __name__ == "__main__":
    t0 = time.time()
    main()
    print(f"Total time: {time.time() - t0:.1f} s")
```


### Merge Transcribed Texts
After running the code above, files like `01.txt`, `02.txt` etc., will be generated according to the naming convention. Use the following code to merge them if needed. Ensure the file list at the bottom (`files_to_merge`) is correct for your needs. The output filename defaults to `merged_output.txt`.

```python
import os
import re
from datetime import timedelta

def parse_timestamp_str(timestamp_str: str) -> float:
    """Parses an HH:MM:SS timestamp string into total seconds."""
    try:
        h, m, s = map(int, timestamp_str.split(':'))
        return h * 3600 + m * 60 + s
    except ValueError:
        print(f"Warning: Could not parse timestamp '{timestamp_str}'. Using 0 seconds.")
        return 0.0

def format_seconds_to_timestamp(seconds: float) -> str:
    """Formats total seconds into an HH:MM:SS timestamp string."""
    # Ensure seconds are non-negative
    seconds = max(0, seconds)
    # Use timedelta for robust handling of time calculations
    td = timedelta(seconds=round(seconds)) # Round to nearest second
    # Format as HH:MM:SS
    hours, remainder = divmod(td.total_seconds(), 3600)
    minutes, seconds = divmod(remainder, 60)
    return f"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}"

def merge_transcriptions(input_files: list, output_file: str):
    """
    Merges multiple transcription files, adjusting timestamps sequentially.

    Args:
        input_files (list): A list of paths to the transcription files to merge,
                            in the desired sequential order.
        output_file (str): The path to save the merged transcription file.
    """
    merged_content = ""
    cumulative_duration = 0.0  # Keep track of the total duration from previous files

    print(f"[*] Starting merge process for files: {input_files}")

    for i, file_path in enumerate(input_files):
        if not os.path.exists(file_path):
            print(f"Warning: File not found: {file_path}. Skipping.")
            continue

        print(f"    Processing file {i+1}/{len(input_files)}: {file_path}")
        last_end_time_in_file = 0.0 # Track the end time within the current file

        try:
            with open(file_path, 'r', encoding="utf-8") as f_in:
                for line_num, line in enumerate(f_in):
                    line = line.strip()
                    if not line: continue # Skip empty lines

                    # Regex to capture HH:MM:SS-HH:MM:SS timestamp and the text
                    match = re.match(r"(\d{2}:\d{2}:\d{2})-(\d{2}:\d{2}:\d{2})\s+(.*)", line)
                    if not match:
                        print(f"Warning: Skipping malformed line {line_num+1} in {file_path}: '{line}'")
                        continue

                    start_time_str, end_time_str, text_content = match.groups()

                    # Parse original timestamps to seconds
                    original_start_sec = parse_timestamp_str(start_time_str)
                    original_end_sec = parse_timestamp_str(end_time_str)

                    # Calculate new timestamps by adding the cumulative duration
                    new_start_sec = original_start_sec + cumulative_duration
                    new_end_sec = original_end_sec + cumulative_duration

                    # Format new timestamps back to HH:MM:SS strings
                    new_start_time_fmt = format_seconds_to_timestamp(new_start_sec)
                    new_end_time_fmt = format_seconds_to_timestamp(new_end_sec)

                    # Append the adjusted line to the merged content
                    merged_content += f"{new_start_time_fmt}-{new_end_time_fmt} {text_content}\n"

                    # Update the latest end time encountered in this file
                    last_end_time_in_file = max(last_end_time_in_file, original_end_sec)

        except Exception as e:
            print(f"Error reading or processing file {file_path}: {e}")
            # Decide if you want to stop merging or continue with the next file
            # For robustness, we'll continue here but print the error.
            continue # Skip to the next file

        # After processing a file, add its duration to the cumulative total
        # Use the last original end time found in that file
        print(f"    Finished processing {file_path}. Duration added: {last_end_time_in_file:.2f} seconds.")
        cumulative_duration += last_end_time_in_file

    # Write the final merged content to the output file
    try:
        with open(output_file, 'w', encoding="utf-8") as f_out:
            f_out.write(merged_content)
        print(f"âœ” Transcriptions successfully merged and saved to {output_file}")
        print(f"[*] Total merged duration (approx): {format_seconds_to_timestamp(cumulative_duration)}")
    except Exception as e:
        print(f"Error writing merged content to {output_file}: {e}")

# --- Example Usage ---
# List the files you want to merge in the correct order
# Ensure these files exist after running the transcription script
files_to_merge = ["01.txt", "02.txt"] # Add more files if needed, e.g., "03.txt"
output_merge_file = "merged_output.txt"

# Check if the input files exist before attempting merge
existing_files = [f for f in files_to_merge if os.path.exists(f)]
if len(existing_files) != len(files_to_merge):
     print(f"[!] Warning: Not all specified files found. Merging only existing files: {existing_files}")

if existing_files:
    merge_transcriptions(existing_files, output_merge_file)
else:
    print("[!] No input files found for merging. Skipping merge operation.")
```

## Contributing

Contributions of any kind are welcome! If you're unsure how to contribute or need adjustments, feel free to ask GPT â€“ I learned much of this by asking questions iteratively myself!

## License

This project is licensed under the [MIT License](LICENSE). Please see the [LICENSE](LICENSE) file for details.

## Third-Party Licenses

- **faster-whisper**: This project uses the [faster-whisper](https://github.com/SYSTRAN/faster-whisper) library, which is licensed under the MIT License.
- **faster-whisper-large-v2-zh-TW Model**: The example model used in this project comes from [Hugging Face](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW). Please adhere to its specific license terms.

## Acknowledgements

Special thanks to the following projects and resources for their support:

- The [faster-whisper](https://github.com/SYSTRAN/faster-whisper) development team
- [Hugging Face](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW) for providing excellent model resources
- [Kaggle](https://www.kaggle.com/) for providing free GPU resources

## Contact

If you have any questions or suggestions, please feel free to reach out:

- **Email**: a0953041880@gmail.com
- **GitHub Issues**: Please use the repository's "Issues" tab for bug reports or feature requests.

Thank you for your interest and support! Cheers~ See you next time.

---

> **Note**: This project makes full use of the free GPU resources provided by Kaggle. Please comply with Kaggle's terms of use, use resources responsibly, and avoid misuse.
