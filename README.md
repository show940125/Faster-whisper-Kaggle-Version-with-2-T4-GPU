# Faster-whisper-Kaggle-Version-with-2-T4-GPU
åª’é«”ã€æ”¿æ²»å·¥ä½œè€…çš„é•·ç¯‡é€å­—ç¨¿æ•‘æ˜Ÿ

<div align="center">
  <a href="#ç¹é«”ä¸­æ–‡" style="margin-right: 20px;">
    <img src="https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-000?style=for-the-badge&logo=translate&logoColor=white" alt="ç¹é«”ä¸­æ–‡">
  </a>
  <a href="#english">
    <img src="https://img.shields.io/badge/English-000?style=for-the-badge&logo=translate&logoColor=white" alt="English">
  </a>
</div>

---
## ç¹é«”ä¸­æ–‡

### æœ¬å°ˆæ¡ˆæä¾›åª’é«”å·¥ä½œè€…ã€æ”¿æ²»å·¥ä½œè€…åŠè‹¦é€¼å·¥è®€ç”Ÿæ“ºè„«ä¸æ•ˆç‡çš„**è½æ‰“å›°å¢ƒ**ï¼Œç‰¹åˆ¥è¨­è¨ˆä¸€å€‹é«˜æ•ˆçš„éŸ³é »è½‰éŒ„çš„å‚»ç“œæ“ä½œæ–¹æ¡ˆã€‚
### ç‚ºä½•ä¸ä½¿ç”¨colabï¼Ÿ
1. å› ç‚ºç²å–GPUçš„é™åˆ¶èˆ‡ä¸ç©©å®šæ€§ï¼Œå³ä¾¿æ›è¼‰driveå¯ä»¥è®“æ•´å€‹ç¨‹åºæ›´å®¹æ˜“ä½¿ç”¨ï¼Œä½†è½‰éŒ„ä»»å‹™çš„é‡é»ä»åœ¨GPUã€‚
2. å› æ­¤ï¼Œç‚ºäº†å……åˆ†åˆ©ç”¨æ‰€æœ‰å…è²»è³‡æºï¼Œä½¿ç”¨ Kaggle å¹³å°å…è²»æä¾›çš„å…©å¼µ T4 GPU è³‡æºï¼Œå¯ä»¥å¤§å¹…æå‡è½‰éŒ„æ•ˆç‡ï¼ŒåŠ©æ‚¨è¼•é¬†å®Œæˆå„é¡è½‰éŒ„ä»»å‹™ã€‚**[æ›´æ–°]** **æ–°ä»£ç¢¼æœƒè‡ªå‹•åµæ¸¬å¯ç”¨çš„ GPU æ•¸é‡ï¼Œä¸¦åˆç†åˆ†é…ä»»å‹™ã€‚**
3. kaggleæ˜¯ä¸€å€‹å…è²»çš„æ©Ÿå™¨å­¸ç¿’å¹³å°ï¼Œç‰¹é»æ˜¯æ¯å€‹æ˜ŸæœŸæä¾›30å€‹å°æ™‚çš„å…è²»GPUï¼Œç›¸è¼ƒæ–¼ä¸ç©©å®šçš„colabï¼Œçµ•å°è¶³ä»¥æ»¿è¶³æ—¥å¸¸çš„å·¥ä½œä»»å‹™ã€‚
### æœ¬äººå·²è„«é›¢æ”¿æ²»å·¥ä½œï¼Œä¹Ÿä¸çŸ¥é“èª°æœƒçœ‹åˆ°æœ¬å°ˆæ¡ˆ(å› ç‚ºé€šå¸¸é€™é¡å·¥ä½œçš„äººæ ¹æœ¬ä¸æœƒæ‰“é–‹github)ï¼Œå¹«åŠ©å¾Œè¼©å°‘èµ°å½è·¯æ˜¯æˆ‘å°å‰ä¸€ä»½å·¥ä½œçš„åŸ·å¿µï¼Œç›®å‰é€™äº›ä»£ç¢¼é›†æˆå·²ç¶“æ¥è¿‘ç©©å®šï¼Œå¾ŒçºŒæ˜¯å¦é‚„æœ‰å„ªåŒ–ç©ºé–“æˆ‘å†å•å•GPT~ **[æ›´æ–°]** **æ–°ä»£ç¢¼æ•´åˆäº†æ‰¹æ¬¡è™•ç† (Batch Processing) å’Œæ›´ç²¾ç´°çš„ä¸¦è¡Œæ§åˆ¶ï¼Œæ•ˆç‡æ›´é«˜ã€‚**
### çš„ç¢ºç¶²è·¯ä¸Šæœ‰æ¯”è¼ƒå¿«çš„Demoï¼Œæ¯”å¦‚whisperJAXã€Whisper web gpuç”šè‡³groq apiç­‰ç­‰ï¼Œä½†ç„¡æ³•ä½¿ç”¨å®¢è£½åŒ–æ¨¡å‹ï¼Œä¹Ÿç„¡æ³•ä½¿ç”¨è¼ƒå¤§çš„éŸ³æª”(é€šå¸¸è¶…é25MB~=30åˆ†é˜ä½éŸ³å€¼mp3æª”å°±ä¸å¤ªèƒ½ç”¨)
### â˜†è´ˆèˆ‡æœ‰ç·£äºº~åæ­£æ•´å¥—ç›®å‰éƒ½ä¸ç”¨èŠ±éŒ¢~
### ~~â˜†æ–°å¢ï¼š**FW1.1.1 with Batched pipeline full code (2024/11/22)~~ **[æ›´æ–°]** **ç›®å‰ä»£ç¢¼å·²æ•´åˆ `BatchedInferencePipeline`ï¼Œä¸¦æä¾›æ›´å®Œå–„çš„è‡ªå‹•åŒ–è™•ç†æµç¨‹ (2025/04/22)ã€‚**

## ç›®éŒ„

- [åŠŸèƒ½](#åŠŸèƒ½)
- [æŠ€è¡“ç‰¹è‰²](#æŠ€è¡“ç‰¹è‰²)
- [å®‰è£èˆ‡è¨­å®š](#å®‰è£èˆ‡è¨­å®š)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [ç¯„ä¾‹ä»£ç¢¼](#ç¯„ä¾‹ä»£ç¢¼)
- [è²¢ç»](#è²¢ç»)
- [æˆæ¬Šæ¢æ¬¾](#æˆæ¬Šæ¢æ¬¾)
- [è‡´è¬](#è‡´è¬)
- [è¯ç¹«æ–¹å¼](#è¯ç¹«æ–¹å¼)

## åŠŸèƒ½

1.  **[æ›´æ–°]** **å¤š GPU è‡ªå‹•åŒ–ä¸¦è¡Œè™•ç†**
    *   **è‡ªå‹• GPU åµæ¸¬èˆ‡åˆ†é…**ï¼šè‡ªå‹•åµæ¸¬ Kaggle ç’°å¢ƒä¸­å¯ç”¨çš„ T4 GPU æ•¸é‡ (æœ€å¤šæ”¯æ´4å€‹)ï¼Œä¸¦ä»¥è¼ªè©¢ (Round-Robin) æ–¹å¼å°‡éŸ³æª”åˆ†é…çµ¦ä¸åŒ GPU è™•ç†ã€‚
    *   **æ‰¹æ¬¡æ¨ç† (Batch Inference)**ï¼šåˆ©ç”¨ `BatchedInferencePipeline` å°‡å¤šå€‹è½‰éŒ„è«‹æ±‚æ‰“åŒ…è™•ç†ï¼Œå¤§å¹…æå‡ GPU åˆ©ç”¨ç‡å’Œæ•´é«”ååé‡ã€‚
    *   **å–® GPU ä¸¦è¡Œæ§åˆ¶**ï¼šå¯è¨­å®šæ¯å¼µ GPU åŒæ™‚è™•ç†çš„æœ€å¤§ä»»å‹™æ•¸ (`MAX_CONCURRENCY_PER_GPU`)ï¼Œé¿å…å–®ä¸€ GPU éè¼‰ï¼Œç¢ºä¿ç©©å®šé‹è¡Œã€‚
    *   **è½‰éŒ„æ–‡æœ¬åˆä½µèˆ‡æ™‚é–“æˆ³è¨˜æ¥çºŒ**ï¼šå°‡å¤šå€‹è½‰éŒ„çµæœåˆä½µç‚ºä¸€å€‹æ–‡ä»¶ï¼Œä¸¦è‡ªå‹•æ¥çºŒæ™‚é–“æˆ³è¨˜ï¼Œç¢ºä¿æ™‚é–“è»¸é€£çºŒã€‚

2.  **æ¨¡å‹æœ¬åœ°åŒ–èˆ‡åŠ è¼‰å„ªåŒ–**
    *   **æ¨¡å‹é å…ˆä¸‹è¼‰èˆ‡æœ¬åœ°åŠ è¼‰**ï¼šæå‰ä¸‹è¼‰é©ç”¨ `faster-whisper` çš„æ¨¡å‹ä¸¦ä¸Šå‚³è‡³ Kaggle çš„ Datasets æˆ– Modelsï¼Œé¿å…æ¯æ¬¡é‹è¡Œæ™‚é‡æ–°ä¸‹è¼‰ï¼Œæé«˜é ç†±é€Ÿåº¦ã€‚
    *   **[æ–°å¢]** **å¤š GPU æ¨¡å‹ç¨ç«‹åŠ è¼‰**ï¼šç‚ºæ¯å€‹åµæ¸¬åˆ°çš„ GPU ç¨ç«‹åŠ è¼‰æ¨¡å‹å¯¦ä¾‹ï¼Œç¢ºä¿ä¸¦è¡Œè™•ç†é †æš¢ã€‚

3.  **è½‰éŒ„æ–‡æœ¬çš„åˆ†æ®µèˆ‡æ‰“å°å„ªåŒ–**
    *   **å›ºå®šæ™‚é–“é–“éš”åˆ†æ®µ**ï¼šåŸºæ–¼å¯é…ç½®çš„ `SEGMENT_DURATION` (é è¨­ 30 ç§’) æ™‚é–“é–“éš”é€²è¡Œæ–‡æœ¬åˆ†æ®µï¼Œæå‡ä¸Šä¸‹æ–‡å¯è®€æ€§ï¼Œæ–¹ä¾¿å¾ŒçºŒæ ¡æ­£ã€‚
    *   **å³æ™‚é€²åº¦æ‰“å°**ï¼šè½‰éŒ„éç¨‹ä¸­ï¼Œé€æ®µæ‰“å°å¸¶æ™‚é–“æˆ³çš„æ–‡æœ¬åˆ°æ§åˆ¶å°ï¼Œæ–¹ä¾¿å³æ™‚ç›£æ§é€²åº¦ã€‚**[æ–°å¢]**
    *   **geminiå¾ŒçºŒæ ¡æ­£**ï¼šåˆ©ç”¨Google AI Studioå¯èª¿ç”¨å…è²»gemini 2.5 /proflashé€²è¡Œé«˜ç²¾åº¦å…è²»æ ¡ç¨¿ï¼Œç¶“å¤šæ¬¡æ¸¬è©¦é©—è­‰ï¼Œæ¯æ¬¡æ ¡æ­£ç´„6500tokenä¹‹ä»½é‡å¯ä»¥å…¼é¡§ç©©å®šçš„æ ¡æ­£å“è³ªä»¥åŠæ•ˆç‡ã€‚

4.  **[æ›´æ–°]** **æ˜“ç”¨æ€§èˆ‡éŒ¯èª¤è™•ç†**
    *   **è‡ªå‹•éŸ³æª”æƒæ**ï¼šè‡ªå‹•éè¿´æƒææŒ‡å®šæ ¹ç›®éŒ„ (`AUDIO_ROOT`) ä¸‹çš„æ‰€æœ‰éŸ³æª” (æ”¯æ´ `.wav`, `.flac`, `.mp3`, `.ogg` ç­‰æ ¼å¼)ï¼Œç„¡éœ€æ‰‹å‹•æŒ‡å®šæ¯å€‹æ–‡ä»¶ã€‚
    *   **è‡ªå‹•è¼¸å‡ºå‘½å**ï¼šæ ¹æ“šæƒæåˆ°çš„éŸ³æª”é †åºï¼Œè‡ªå‹•ç”Ÿæˆ `01.txt`, `02.txt`... ç­‰è¼¸å‡ºæª”åã€‚
    *   **åƒæ•¸é›†ä¸­ç®¡ç†**ï¼šå°‡å¸¸ç”¨åƒæ•¸ (æ¨¡å‹è·¯å¾‘ã€éŸ³æª”æ ¹ç›®éŒ„ã€æ‰¹æ¬¡å¤§å°ã€ä¸¦è¡Œæ•¸ã€æ›¿æ›è©ç­‰) é›†ä¸­åœ¨ä»£ç¢¼é–‹é ­ï¼Œæ–¹ä¾¿ä¿®æ”¹ã€‚
    *   **éŒ¯èª¤æ•æ‰**ï¼šå°å–®å€‹æ–‡ä»¶çš„è½‰éŒ„éç¨‹é€²è¡ŒéŒ¯èª¤æ•æ‰ï¼Œé¿å…å–®ä¸€æ–‡ä»¶å¤±æ•—å°è‡´æ•´å€‹ç¨‹åºä¸­æ–·ã€‚

5.  **æ™‚é–“æˆ³è¨˜ä½ç½®èˆ‡æ ¼å¼èª¿æ•´**
    *   **æ™‚é–“æˆ³è¨˜ç½®æ–¼æ®µè½é–‹é ­**ï¼šçµ±ä¸€æ™‚é–“æˆ³è¨˜æ ¼å¼ (`HH:MM:SS-HH:MM:SS`)ï¼Œç¢ºä¿æ¯å€‹æ®µè½çš„æ™‚é–“æˆ³è¨˜ä½æ–¼æ–‡æœ¬æœ€å‰ç«¯ã€‚

## æŠ€è¡“ç‰¹è‰²

- **[æ›´æ–°]** **é«˜æ•ˆæ‰¹æ¬¡æ¨ç†**ï¼šåˆ©ç”¨ `BatchedInferencePipeline` å¯¦ç¾é«˜æ•ˆæ‰¹æ¬¡è™•ç†ï¼Œæœ€å¤§åŒ– GPU ååé‡ã€‚
- **[æ›´æ–°]** **è‡ªå‹•åŒ–å¤š GPU ç®¡ç†**ï¼šè‡ªå‹•åµæ¸¬ä¸¦åˆ©ç”¨æ‰€æœ‰å¯ç”¨ GPU (æœ€å¤š4å€‹)ï¼Œæ™ºèƒ½åˆ†é…ä»»å‹™ã€‚
- **[æ›´æ–°]** **ç²¾ç´°åŒ–ä¸¦è¡Œæ§åˆ¶**ï¼šé€šé `threading.Semaphore` æ§åˆ¶å–®å¼µ GPU çš„ä¸¦è¡Œä»»å‹™æ•¸ï¼Œå¹³è¡¡æ•ˆç‡èˆ‡ç©©å®šæ€§ã€‚
- **[æ›´æ–°]** **è‡ªå‹•åŒ–æ–‡ä»¶è™•ç†**ï¼šè‡ªå‹•æƒæéŸ³æª”ã€è‡ªå‹•å‘½åè¼¸å‡ºæ–‡ä»¶ï¼Œç°¡åŒ–æ“ä½œæµç¨‹ã€‚
- **é«˜æ•ˆçš„å¤šåŸ·è¡Œç·’è™•ç†**ï¼šä½¿ç”¨ `concurrent.futures.ThreadPoolExecutor` å¯¦ç¾å¤šåŸ·è¡Œç·’èª¿åº¦ã€‚~~ç¶“æ¸¬è©¦ï¼Œå¤šé€²ç¨‹æ•ˆç‡ä½æ–¼å¤šç·šç¨‹(2024/12/11ï¼ŒFW=1.1.0)~~ **[è¨»]** **æ–°ä»£ç¢¼ä¾ç„¶ä½¿ç”¨å¤šç·šç¨‹ï¼Œçµåˆ Semaphore æ§åˆ¶ä¸¦è¡Œåº¦ã€‚**
- **å„ªåŒ–çš„æ¨¡å‹åŠ è¼‰**ï¼šå°‡ `faster-whisper` é©ç”¨çš„æ¨¡å‹ä¸Šå‚³ Kaggleï¼Œé¿å…æ¯æ¬¡é‡è¤‡ä¸‹è¼‰ï¼Œæ¸›å°‘è½‰éŒ„ä»»å‹™å‰çš„æº–å‚™æ™‚é–“ã€‚
- **éˆæ´»çš„æ–‡æœ¬åˆ†æ®µæ–¹å¼**ï¼šæ”¯æŒå›ºå®šæ™‚é–“é–“éš” (`SEGMENT_DURATION`) åˆ†æ®µæ–¹å¼ï¼Œæå‡è½‰éŒ„æ–‡æœ¬çš„å¯è®€æ€§ã€‚
- **è‡ªå‹•åŒ–çš„è½‰éŒ„æ–‡æœ¬åˆä½µ**ï¼šè‡ªå‹•æ¥çºŒæ™‚é–“æˆ³è¨˜ï¼Œç¢ºä¿å¤šå€‹è½‰éŒ„æ–‡ä»¶çš„æ™‚é–“è»¸é€£çºŒã€‚

## å®‰è£èˆ‡è¨­å®š

### ç’°å¢ƒéœ€æ±‚
- **Python ç‰ˆæœ¬**: 3.8+
- **å¹³å°**: Kaggleï¼ˆKaggle å·²ç¶“ç‚ºä½ é å…ˆé…ç½®äº†æ‰€éœ€ç’°å¢ƒï¼‰
- **ç¡¬é«”**: å…©å¼µ T4 GPUï¼ˆç”± Kaggle æä¾›å…è²»è³‡æºï¼‰

### é‹è¡Œæ­¥é©Ÿ

#### 1. å®‰è£å¿…è¦çš„ Python å¥—ä»¶

åœ¨æ–°é–‹çš„ Kaggle Notebook ä¸­ï¼ŒåŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä¾†å®‰è£ `faster-whisper` å¥—ä»¶ï¼š(ç´„20ç§’)
#### 2024/10/26**æ›´æ–°**:ctranslate2æœ€æ–°ç‰ˆåœ¨cudaç›¸å®¹æ€§ä¸Šè²Œä¼¼å‡ºç¾å•é¡Œï¼Œç›®å‰ä»¥é€€å›ç‰ˆæœ¬æ–¹å¼è™•ç†
#### 2024/12/11**æ›´æ–°**:~FW1.1.0ç‰ˆæœ¬ä¸­å•é¡Œä¼¼ä¹å·²ç¶“è§£æ±ºï¼Œå¯ä»¥æ‹¿æ‰ctranslate2==4.4.0ã€‚~
#### 2025/04/22**æ›´æ–°**:FW1.1.1ç‰ˆæœ¬ä¸‹å•é¡Œå—é™æ–¼å¹³å°ç’°å¢ƒçš„ä¾è³´ç‰ˆæœ¬ï¼Œç›®å‰ä»éœ€ctranslate2==4.4.0ã€‚ **[è¨»]**:å¹³å°ç’°å¢ƒç›®å‰ä¸ç©©å®šï¼Œå»ºè­°å›æ»¾åˆ°å»å¹´ä»¥å‰çš„ç’°å¢ƒæˆ–ç­‰æ›´æ–°

```python
!pip install faster-whisper==1.1.1 ctranslate2==4.4.0 -q
# ä½¿ç”¨ -q å®‰éœæ¨¡å¼å®‰è£
```

#### 2. ä¸Šå‚³æ¨¡å‹è‡³ Kaggleï¼Œä¸¦åœ¨notebookä¸­åŠ è¼‰

æ¥ä¸‹ä¾†ï¼Œå°‡ `faster-whisper` æ¨¡å‹å¾ Hugging Face ä¸‹è¼‰ï¼Œä¸¦ä¸Šå‚³è‡³ Kaggleã€‚è«‹éµå¾ªä»¥ä¸‹æ­¥é©Ÿï¼š

1. å‰å¾€ Hugging Face æ¨¡å‹é é¢ï¼šç¯„ä¾‹[faster-whisper-large-v2-zh-TW](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW)
2. é»æ“Šé é¢ä¸­çš„ **Files and version** æŒ‰éˆ•ï¼Œå°‡æ¨¡å‹çš„æ‰€æœ‰æª”æ¡ˆä¸‹è¼‰åˆ°æœ¬åœ°é›»è…¦ã€‚
3. ç™»å…¥ Kaggleï¼Œä¸¦åœ¨é é¢ä¸­é»æ“Š**creat**ï¼Œé¸æ“‡ **Create New model**ã€‚
4. åœ¨å»ºç«‹æ–°çš„ model æ™‚ï¼Œç‚ºå®ƒå–ä¸€å€‹åç¨±ï¼Œä¾‹å¦‚ï¼š`faster-whisper-large-v2-zh-TW`ã€‚
5. å°‡ä¸‹è¼‰å¥½çš„æ‰€æœ‰æª”æ¡ˆä¸Šå‚³åˆ°è©² model è³‡æ–™å¤¾ä¸­ã€‚
6. ä¸Šå‚³å®Œæˆå¾Œï¼Œé»æ“Š **Create** æŒ‰éˆ•ï¼Œå³å¯å®Œæˆæ¨¡å‹ä¸Šå‚³è‡³ Kaggleã€‚
7. é¦–æ¬¡ä½¿ç”¨é …ç›®æ™‚ï¼Œé»æ“Šå³é‚Š"input"ä¸‹çš„"add input"ï¼Œé¸"models"ä»¥åŠ"your work"æ­£ç¢ºæ‰¾åˆ°ä½ ä¸Šå‚³å¥½çš„modelï¼Œä¹‹å¾Œå†å•Ÿå‹•notebokkå°±æœƒè‡ªå‹•åŠ è¼‰(æ‹æ‰‹~)

å‚™è¨»ï¼šlargeV3æ¨¡å‹å¾®èª¿çš„modelåœ¨è½‰éŒ„é€Ÿåº¦ä¸Šæœƒæ¯”largeV2çš„å¿«10%~20%ï¼Œä½†ç²¾åº¦ä¸Šï¼ŒåŸºæ–¼largeV2å¾®èª¿çš„æ¨¡å‹å°æ–¼ä¸­æ–‡çš„é©æ‡‰æ€§è¼ƒä½³ï¼Œç¯„ä¾‹æä¾›çš„ç‰ˆæœ¬å·²ç¶“æ˜¯ç²¾åº¦æœ€ä½³çš„ç‰ˆæœ¬ä¹‹ä¸€ã€‚

####  [æ›´æ–°] ä¸Šå‚³éŸ³æª”è‡³ Kaggle Dataset


1. å¾ä½ çš„è£ç½®å°‡æ¬²è½‰éŒ„çš„éŸ³æª”åˆ‡ä¸€åŠï¼Œä½ å¯ä»¥ç”¨ffmpegæˆ–æ˜¯ç”¨å‰ªæ˜ éƒ½å¯
2. è·Ÿä¸Šå‚³æ¨¡å‹æ­¥é©Ÿé¡ä¼¼ï¼Œåªæ˜¯è¦é¸æ“‡datasetï¼Œä½ å¯ä»¥åœ¨åŒä¸€å€‹datasetä¸­ä¸Šå‚³è¤‡æ•¸å€‹éŸ³æª”
3. ä½¿ç”¨notebookçš„æ™‚å€™"add input"å°‡éŸ³æª”åŒ¯å…¥notebookä¸­ï¼Œæ–¹æ³•è·Ÿmodelä¸€æ¨£
4. ä¹‹å¾Œä»£ç¢¼å°è‡ªå‹•æ•æ‰éŸ³æª”ï¼Œä¸ç”¨ç®¡ä»–


## ä½¿ç”¨æ–¹æ³•

åœ¨æ–°é–‹çš„kaggle notebookä¸­ï¼Œæ ¹æ“šä»¥ä¸‹ä»£ç¢¼é€æ­¥åŸ·è¡Œï¼Œç­‰æ–¼å‚»ç“œæ“ä½œå–”~

## ç¯„ä¾‹ä»£ç¢¼
(è«‹æŒ‰ä»£ç¢¼å¡Šè¤‡è£½è²¼ä¸Šå°±è¡Œï¼Œè®“ä½ æ”¹ä½ å†æ”¹)

### è½‰éŒ„éŸ³é »æ–‡ä»¶
1. æ³¨æ„MODEL_PATH = "è¤‡è£½ä½ inputçš„whisper modelçš„è·¯å¾‘(ç›´æ¥é»COPYå°±å¥½)"
2. å…¶ä»–åƒæ•¸å·²ç¶“ç¶“éè¶…é300å°æ™‚ä»¥ä¸Šçš„è½‰éŒ„ä»»å‹™å¯¦éš›é©—è­‰ï¼Œä¸å¤ªéœ€è¦èª¿æ•´
3. replacements(ç”¨Ctrl+fæŸ¥æ‰¾)éƒ¨åˆ†èƒ½æä¾›å›ºå®šè½‰éŒ„ä»»å‹™(æ¯”å¦‚å¤šæ¬¡è½‰éŒ„åŒä¸€å€‹è¬›è€…çš„éŸ³æª”)è¼ƒä½³çš„é«”é©—ï¼ŒæŠŠå›ºå®šçš„éŒ¯æ¼å­—ç›´æ¥æ›¿æ›æˆæ­£ç¢ºçš„æ–‡å­—ï¼Œæ ¼å¼ç‚º"éŒ¯å­—": "æ­£ç¢ºå­—",
4. æœ¬å°ˆæ¡ˆ(æŒ‰ç¯„ä¾‹æ¨¡å‹)å¯¦æ¸¬3å°æ™‚éŸ³é »æ–‡ä»¶(WAVæª”ï¼ŒåŒå¸¸å¤§å°ç´„300MB)çš„è½‰éŒ„ä»»å‹™ç´„éœ€9åˆ†é˜è½‰éŒ„æ™‚é–“ï¼Œæº–ç¢ºåº¦å¹³å‡åœ¨90%ä»¥ä¸Šï¼Œå†é€égeminiæ ¡æ­£ï¼Œæº–ç¢ºç‡å¯é”99%ï¼Œä¸å”¬çˆ›ã€‚
5. éŒ„éŸ³ç­†å»ºè­°é è¨­éŒ„è£½WAV/flacæª”ï¼Œç²¾åº¦ç¢ºå¯¦å„ªæ–¼MP3æª”ã€‚WAVæª”(192K&256K)å¤§æ¦‚æ˜¯éŸ³è³ªå½±éŸ¿ç²¾åº¦çš„æ¥µé™ï¼Œå†å¤§å‰‡ç„¡ç”¨ã€‚

```python
# ---------- importå­—å…¸ ----------
from faster_whisper import WhisperModel, BatchedInferencePipeline
import datetime, time, os, re, torch, glob
from typing import List, Tuple, Dict
import concurrent.futures, threading

# ---------- å¯èª¿åƒæ•¸ ----------
MODEL_PATH = "/kaggle/input/faster-whisper..."
AUDIO_ROOT = "/kaggle/input"            # åªæ”¹é€™è£¡å°±èƒ½æ›è³‡æ–™ä¾†æº
AUDIO_EXTS = (".wav", ".flac", ".mp3", ".ogg")   # å…è¨±çš„éŸ³æª”å‰¯æª”å
SEGMENT_DURATION = 30.0                          # æ¯æ®µæœ€é•·ç§’æ•¸
BATCH_SIZE = 8
MAX_CONCURRENCY_PER_GPU = 2                     # åŒå¼µå¡ä¸¦è¡Œä¸Šé™
REPLACEMENTS: Dict[str, str] = {                # å¸¸è¦‹éŒ¯å­—ä¿®æ­£è¡¨
    "XX": "OO" 
}
INITIAL_PROMPT = "XXX"                      # çµ¦æ¨¡å‹çš„ system promptï¼ˆå¯ç•™ç©ºï¼‰

# ---------- è‡ªå‹•æ”¶é›†éŸ³æª” ----------
def collect_audio_files(root: str, exts=AUDIO_EXTS) -> List[str]:
    """
    éè¿´èµ°è¨ª root åº•ä¸‹æ‰€æœ‰å­ç›®éŒ„ï¼Œ
    åªè¦æª”åå‰¯æª”åï¼ˆä¸è«–å¤§å°å¯«ï¼‰ç¬¦åˆ extsï¼Œå°±æ”¶é€²ä¾†ã€‚
    """
    exts_lower = {e.lower() for e in exts}
    files = []
    for dirpath, _, filenames in os.walk(root):
        for fn in filenames:
            ext = os.path.splitext(fn)[1].lower()
            if ext in exts_lower:
                files.append(os.path.join(dirpath, fn))
    return sorted(files)

# ---------- å»ºç«‹ (éŸ³æª”, è¼¸å‡ºæª”, GPU index) å°ç…§è¡¨ ----------
def create_job_table(audio_files: List[str], gpu_count: int) -> List[Tuple[str, str, int]]:
    jobs = []
    for idx, path in enumerate(audio_files, start=1):
        out_name = f"{idx:02d}.txt"
        gpu_idx = idx % gpu_count  # roundâ€‘robin
        jobs.append((path, out_name, gpu_idx))
    return jobs

# ---------- å–ä»£/æ¸…æ´—å·¥å…· ----------
pattern = re.compile("|".join(re.escape(k) for k in REPLACEMENTS.keys()))
def clean_text(txt: str) -> str:
    txt = txt.lstrip("! ")
    return pattern.sub(lambda m: REPLACEMENTS[m.group(0)], txt)

def to_timestamp(sec: float) -> str:
    h = int(sec // 3600)
    m = int((sec % 3600) // 60)
    s = int(sec % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"

def fmt_chunk(start: float, end: float, txt: str) -> str:
    return f"{to_timestamp(start)}-{to_timestamp(end)} {txt.strip()}\n"

# ---------- è½‰éŒ„ + å¯«æª” ----------
def process_segments(segments, outfile: str, max_len=SEGMENT_DURATION):
    buf, chunk_start, chunk_txt = "", 0.0, ""
    for seg in segments:
        chunk_txt += " " + clean_text(seg.text)
        if seg.end - chunk_start >= max_len:
            line = fmt_chunk(chunk_start, seg.end, chunk_txt)
            print(line, end="", flush=True)
            buf += line
            chunk_start, chunk_txt = seg.end, ""
    if chunk_txt:
        line = fmt_chunk(chunk_start, seg.end, chunk_txt)
        print(line, end="", flush=True)
        buf += line
    with open(outfile, "w", encoding="utfâ€‘8") as fh:
        fh.write(buf)
    print(f" âœ” å·²å¯«å…¥ {outfile}")

def transcribe_single(job, pipelines, semaphores):
    in_path, out_path, gpu_idx = job
    sem = semaphores[gpu_idx]
    with sem:  # é™åˆ¶åŒä¸€å¼µå¡çš„ä¸¦è¡Œæ•¸
        try:
            segments, _info = pipelines[gpu_idx].transcribe(
                in_path,
                batch_size=BATCH_SIZE,
                word_timestamps=True,
                hallucination_silence_threshold=3,
                initial_prompt=INITIAL_PROMPT or None,
                beam_size=5,
                temperature=0,
                patience=1.5,
                language="zh",
                max_new_tokens=256,
                condition_on_previous_text=False,
                no_repeat_ngram_size=3,
                vad_filter=True,
                vad_parameters={"min_silence_duration_ms": 250, "speech_pad_ms": 600},
                log_progress=True,
            )
            process_segments(segments, out_path)
        except Exception as exc:
            print(f" âœ˜ è½‰éŒ„å¤±æ•—: {in_path} ({exc})")

# ---------- ä¸»æµç¨‹ ----------
def main():
    # 1. æª¢æŸ¥ GPU
    gpu_count = torch.cuda.device_count() or 1   # æ²’ GPU æ™‚ fallback CPU
    if gpu_count > 4:                            # Kaggle é€šå¸¸ 1 å¼µå¡ï¼›é€™è£¡åªæ˜¯ä¿éšª
        gpu_count = 4
    print(f"åµæ¸¬åˆ° GPU æ•¸é‡ï¼š{gpu_count}")
    
    # 2. æƒæéŸ³æª”
    audio_files = collect_audio_files(AUDIO_ROOT)
    if not audio_files:
        raise RuntimeError(f"æ‰¾ä¸åˆ°ä»»ä½•éŸ³æª”æ–¼ {AUDIO_ROOT}")
    job_table = create_job_table(audio_files, gpu_count)
    
    # 3. å»ºç«‹æ¯å¼µå¡å„è‡ªçš„æ¨¡å‹èˆ‡ pipeline
    pipelines = {}
    for idx in range(gpu_count):
        dev = "cuda" if torch.cuda.is_available() else "cpu"
        pipelines[idx] = BatchedInferencePipeline(
            WhisperModel(MODEL_PATH, device=dev, device_index=idx, compute_type="float16")
        )
        print(f"GPU {idx} æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    # 4. ç‚ºæ¯å¼µå¡æº–å‚™ Semaphoreï¼Œæ§åˆ¶ä¸¦è¡Œåº¦
    semaphores = {idx: threading.Semaphore(MAX_CONCURRENCY_PER_GPU) for idx in range(gpu_count)}
    
    # 5. å¤šåŸ·è¡Œç·’ä¸¦è¡Œè½‰éŒ„
    workers = gpu_count * MAX_CONCURRENCY_PER_GPU
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as pool:
        futures = [pool.submit(transcribe_single, job, pipelines, semaphores) for job in job_table]
        for f in concurrent.futures.as_completed(futures):
            pass  # éŒ¯èª¤å·²åœ¨ transcribe_single å…§æ•æ‰
    
    print("ğŸ‰ æ‰€æœ‰è½‰éŒ„ä»»å‹™å®Œæˆ")

# Entry
if __name__ == "__main__":
    audio_list = collect_audio_files(AUDIO_ROOT)
    print(f"å…±æ‰¾åˆ° {len(audio_list)} å€‹éŸ³æª”ï¼Œå‰ 10 ç­†ï¼š")
    for p in audio_list[:10]:
        print("  ", p)
    tic = time.time()
    main()
    print(f"ç¸½è€—æ™‚ï¼š{time.time() - tic:.1f} ç§’")
```

### åˆä½µè½‰éŒ„æ–‡æœ¬
æ‰¿ä¸Šä»£ç¢¼ï¼Œé å‘½åè¦å‰‡ç‚º01.txt->02.txtï¼Œå¦‚éœ€åˆä½µè«‹ä½¿ç”¨ä»¥ä¸‹ä»£ç¢¼ï¼Œç¢ºå®šæœ€åº•ä¸‹çš„çµ„åˆæ˜¯å¦æ­£ç¢ºï¼Œå³å¯æŒ‰éœ€åˆä½µå›‰~(æª”åé è¨­ç‚ºmerged_output.txt)
```python
def merge_transcriptions(file1, file2, output_file):
    def parse_timestamp(timestamp_str):
        """å°‡ XX:XX:XX æ ¼å¼çš„æ™‚é–“æˆ³è¨˜è½‰æ›ç‚ºç§’æ•¸"""
        h, m, s = map(int, timestamp_str.split(':'))
        return h * 3600 + m * 60 + s

    def format_timestamp(seconds):
        """å°‡ç§’æ•¸è½‰æ›ç‚º XX:XX:XX æ ¼å¼çš„æ™‚é–“æˆ³è¨˜"""
        return format_to_custom_timestamp(seconds)
    
    merged_content = ""
    total_duration = 0

    # è®€å–ç¬¬ä¸€å€‹æ–‡ä»¶çš„å…§å®¹
    with open(file1, 'r', encoding="utf-8") as f1:
        for line in f1:
            # å‡è¨­æ™‚é–“æˆ³è¨˜åœ¨è¡Œçš„é–‹é ­ï¼Œä¸¦ä¸”æ ¼å¼ç‚º XX:XX:XX-XX:XX:XX
            time_range, text = line.split(' ', 1)
            start_time_str, end_time_str = time_range.split('-')
            
            # å°‡æ™‚é–“æˆ³è¨˜è½‰æ›ç‚ºç§’æ•¸
            start_time = parse_timestamp(start_time_str)
            end_time = parse_timestamp(end_time_str)
            
            # æ›´æ–°æ™‚é–“æˆ³è¨˜ä»¥åŒ…æ‹¬ç´¯ç©çš„ç¸½æ™‚é–“
            new_start_time = format_timestamp(start_time + total_duration)
            new_end_time = format_timestamp(end_time + total_duration)
            
            # åˆä½µæ›´æ–°å¾Œçš„è¡Œ
            merged_content += f"{new_start_time}-{new_end_time} {text}"
        
        # æ›´æ–°ç´¯ç©çš„ç¸½æ™‚é–“
        total_duration = parse_timestamp(new_end_time)
    
    # è®€å–ç¬¬äºŒå€‹æ–‡ä»¶çš„å…§å®¹ä¸¦åˆä½µ
    with open(file2, 'r', encoding="utf-8") as f2:
        for line in f2:
            time_range, text = line.split(' ', 1)
            start_time_str, end_time_str = time_range.split('-')
            
            # å°‡æ™‚é–“æˆ³è¨˜è½‰æ›ç‚ºç§’æ•¸
            start_time = parse_timestamp(start_time_str)
            end_time = parse_timestamp(end_time_str)
            
            # æ›´æ–°æ™‚é–“æˆ³è¨˜ä»¥åŒ…æ‹¬ç´¯ç©çš„ç¸½æ™‚é–“
            new_start_time = format_timestamp(start_time + total_duration)
            new_end_time = format_timestamp(end_time + total_duration)
            
            # åˆä½µæ›´æ–°å¾Œçš„è¡Œ
            merged_content += f"{new_start_time}-{new_end_time} {text}"
    
    # å°‡åˆä½µå¾Œçš„å…§å®¹ä¿å­˜åˆ°è¼¸å‡ºæ–‡ä»¶
    with open(output_file, 'w', encoding="utf-8") as out_file:
        out_file.write(merged_content)

    print(f"Transcriptions merged and saved to {output_file}")

# å‡è¨­å·²ç¶“æˆåŠŸç”Ÿæˆäº† 01.txt å’Œ 02.txt
merge_transcriptions("01.txt", "02.txt", "merged_output.txt")
```

## è²¢ç»

æ­¡è¿ä»»ä½•å½¢å¼çš„è²¢ç»ï¼ä¸æœƒæˆ–è¦åšèª¿æ•´è‡ªå·±å•GPTå–”~æˆ‘ä¹Ÿæ˜¯æ…¢æ…¢å•å‡ºä¾†çš„~

## æˆæ¬Šæ¢æ¬¾

æœ¬å°ˆæ¡ˆæ¡ç”¨ [MIT License](LICENSE) æˆæ¬Šï¼Œè©³æƒ…è«‹åƒé–± [LICENSE](LICENSE) æ–‡ä»¶ã€‚


## ç¬¬ä¸‰æ–¹è³‡æºæˆæ¬Š

- **faster-whisper**ï¼šæœ¬å°ˆæ¡ˆä½¿ç”¨ [faster-whisper](https://github.com/SYSTRAN/faster-whisper) åº«ï¼Œè©²åº«æ¡ç”¨ MIT æˆæ¬Šã€‚
- **faster-whisper-large-v2-zh-TW æ¨¡å‹**ï¼šæœ¬å°ˆæ¡ˆç¯„ä¾‹æ¨¡å‹ä½¿ç”¨ä¾†è‡ª [Hugging Face](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW) çš„æ¨¡å‹ï¼Œè«‹éµå®ˆå…¶æˆæ¬Šæ¢æ¬¾ã€‚

## è‡´è¬

æ„Ÿè¬ä»¥ä¸‹é …ç›®å’Œè³‡æºå°æœ¬å°ˆæ¡ˆçš„æ”¯æŒï¼š

- [faster-whisper](https://github.com/SYSTRAN/faster-whisper) é–‹ç™¼åœ˜éšŠ
- [Hugging Face](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW) æä¾›çš„å„ªç§€æ¨¡å‹è³‡æº
- [Kaggle](https://www.kaggle.com/) æä¾›çš„å…è²» GPU è³‡æº

## è¯ç¹«æ–¹å¼

å¦‚æœ‰ä»»ä½•å•é¡Œæˆ–å»ºè­°ï¼Œè«‹é€šéä»¥ä¸‹æ–¹å¼è¯ç¹«ï¼š

- **é›»å­éƒµä»¶**ï¼ša0953041880@gmail.com
- **GitHub Issues**ï¼šç•¥

æ„Ÿè¬æ‚¨çš„é—œæ³¨èˆ‡æ”¯æŒï¼é†¬~ä¸‹æ¬¡è¦‹

## English

### Welcome to **show940125**'s GitHub repository! This project offers media professionals, political workers, and diligent student assistants a solution to overcome the inefficiency of **transcription challenges**. It is a well-designed and user-friendly audio transcription method.

### Why Not Use Colab?

1. **GPU Availability and Stability**: Colab imposes limitations and instability in accessing GPUs. Even though mounting Google Drive can simplify the workflow, the core of transcription tasks relies heavily on GPU performance.
2. **Maximizing Free Resources**: To fully utilize all available free resources, this project leverages Kaggle's two free T4 GPUs, significantly enhancing transcription efficiency and enabling you to effortlessly complete various transcription tasks.
3. **Reliable Free GPU Access**: Kaggle is a free machine learning platform that offers 30 hours of free GPU usage each week. Compared to the unreliable nature of Colab, Kaggle's offerings are more than sufficient to meet daily work requirements.

### Personal Note

Having moved away from a career in political work, I understand that individuals in such roles might not frequently visit GitHub. My dedication to helping future generations avoid the pitfalls of my previous job is the driving force behind this project. The current codebase is nearing stability, and any potential optimizations will be explored with the help of GPT in the future.

### Existing Online Demos

There are faster demos available online, such as whisperJAX, Whisper Web GPU, and even Groq API. However, these solutions lack the ability to use customized models and cannot handle larger audio files (typically over 25MB or approximately 30 minutes of low-bitrate MP3 files) easily.

### â˜† A Gift for the Worthy~

This entire setup is currently free of charge, and that is the point.

## Table of Contents

- [Features](#features)
- [Technical Highlights](#technical-highlights)
- [Installation and Setup](#installation-and-setup)
- [Usage](#usage)
- [Code Examples](#code-examples)
- [Contributing](#contributing)
- [License](#license)
- [Third-Party Licensing](#third-party-licensing)
- [Acknowledgements](#acknowledgements)
- [Contact](#contact)

## Features

1. **Dual GPU Parallel Processing and Merging Functionality Expansion**
   - **Dual GPU Parallel Processing**: Supports simultaneous processing of two audio files using dual T4 GPUs, significantly enhancing transcription efficiency.
   - **Transcription Text Merging and Timestamp Continuity**: Merges multiple transcription results into a single file with automatic timestamp continuation, ensuring a continuous timeline.

2. **Model Localization and Loading Optimization**
   - **Pre-downloaded Model and Local Loading**: Pre-download models compatible with `faster-whisper` and upload them to Kaggle's module to avoid re-downloading each run, improving preheat speed.

3. **Text Segmentation and Printing Optimization**
   - **Fixed Time Interval Segmentation**: Segments text based on fixed 30-second intervals to enhance readability.
   - **Hybrid Natural and Fixed Segmentation**: Combines natural sentence segmentation with fixed time intervals to maintain sentence coherence and paragraph readability.

4. **Resolution of Repeated Sentences Issue**
   - **Corrected Paragraph Accumulation and Clearing Logic**: Prevents the occurrence of repeated sentences, ensuring clean text output.

5. **Timestamp Position and Format Adjustment**
   - **Timestamps at the Beginning of Paragraphs**: Standardizes timestamp format, ensuring each paragraph's timestamp is at the very beginning of the text.

## Technical Highlights

- **Dual GPU Parallel Processing**: Fully utilizes Kaggleâ€™s two T4 GPUs, doubling transcription efficiency.
- **Efficient Multithreading**: Implements multithreading using `concurrent.futures.ThreadPoolExecutor` to maximize GPU resource utilization.
- **Optimized Model Loading**: Localizes `faster-whisper` compatible models on Kaggle, avoiding repeated downloads and reducing preparation time to under 1 minute.
- **Flexible Text Segmentation**: Supports both fixed time interval and natural sentence segmentation methods to improve transcription text readability.
- **Automated Transcription Merging**: Automatically continues timestamps to ensure a continuous timeline across multiple transcription files.

## Installation and Setup

### Environment Requirements
- **Python Version**: 3.8+
- **Platform**: Kaggle (pre-configured environment)
- **Hardware**: Two T4 GPUs (provided free by Kaggle)

### Running Steps

#### 1. Install Required Python Packages

In a new Kaggle Notebook, execute the following command to install the `faster-whisper` package (approximately 20 seconds):

```python
%%time
pip install faster-whisper==1.1.1
```

#### 2. Upload Model to Kaggle and Load in Notebook

Next, download the `faster-whisper` model from Hugging Face and upload it to Kaggle. Follow these steps:

1. **Visit the Hugging Face Model Page**: Example [faster-whisper-large-v2-zh-TW](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW)
2. **Download Model Files**: Click the **Files and versions** button on the page to download all model files to your local computer.
3. **Upload to Kaggle**:
   - Login to Kaggle and click **Create** > **Create New Model**.
   - Name the model, for example, `faster-whisper-large-v2-zh-TW`.
   - Upload all downloaded files to the model's directory.
   - After uploading, click **Create** to complete the model upload to Kaggle.
4. **Add Model to Notebook**:
   - When first using the project, click **Add Input** under the "Input" section, select **Models**, and locate your uploaded model under "your work".
   - Once added, the notebook will automatically load the model. (Applause~)

**Note**: The `largeV3` model is 10%â€“20% faster than `largeV2` in transcription speed, but the `largeV2` model fine-tuned for Chinese offers better accuracy. The provided example uses one of the most accurate versions available.

#### 3. Upload Audio Files to Kaggle and Load in Notebook

1. **Split Audio Files**: Divide your audio files in half using tools like `ffmpeg` or any audio editing software.
2. **Upload to Dataset**: Similar to uploading the model, but select **Dataset**. You can upload multiple audio files within the same dataset.
3. **Import Audio Files into Notebook**:
   - In the notebook, click **Add Input** to import the audio files, following the same method as uploading the model.
4. **Copy Audio File Paths**: **Important**: In the subsequent code, copy (Ctrl + C) the paths of the audio files to the designated locations.

## Usage

In a new Kaggle Notebook, follow the steps below by executing the provided code blocks.

## Code Examples
(Copy and paste the code blocks, make adjustments as needed)

### Transcribing Audio Files

1. **Set `MODEL_PATH`**: Copy the path of your input Whisper model (simply click COPY).
2. **Other Parameters**: These have been validated through over 300 hours of transcription tasks and generally do not require adjustments.
3. **Replacements**: The `replacements` section (use Ctrl + F to find) provides a better experience for fixed transcription tasks (e.g., transcribing the same speaker multiple times). Directly replace fixed misspelled words with correct ones in the format `"incorrect_word": "correct_word"`.
4. **Audio Path Settings**: Set the audio paths in the two paths under `def main():`, with the first path at the top and the second at the bottom. The resulting two TXT files are preset as `04.txt` and `05.txt`. Due to the need for subsequent merging, renaming is not recommended; otherwise, rename synchronously during merging.
5. **Performance**: This project (using the example model) has been tested with a 3-hour audio file (WAV format, approximately 300MB). The transcription task takes about 9 minutes, with an average accuracy above 95% in chinese. Through replacement rules, accuracy can reach 99%, no exaggeration.
6. **Recording Recommendations**: It is recommended to use WAV files for transcrbing, as they offer better accuracy than MP3 files. WAV files (192K & 256K) are approximately the limit of audio quality affecting accuracy; larger files are ineffective.

```python
from faster_whisper import WhisperModel
import datetime
import os
import logging
import concurrent.futures
from typing import List, Tuple

# Configure Logging
logging.basicConfig()
logging.getLogger("faster_whisper").setLevel(logging.DEBUG)

import concurrent.futures
from typing import List, Tuple

# Set constants
MODEL_PATH = "/kaggle/input/faster-whisper-large-v2-zh-tw/transformers/default/1"
SEGMENT_DURATION = 30.0
MAX_WORKERS = 2

# Define multiple audio files
files: List[Tuple[str, str, int]] = [
    ("/kaggle/input/ccl07-08/CLL08_part1.wav", "04.txt", 0),
    ("/kaggle/input/ccl07-08/CLL08_part2.wav", "05.txt", 1)
]

def transcribe_audio(input_file: str, output_file: str, device_index: int, model, segment_duration: float = SEGMENT_DURATION) -> None:
    # Use the provided model to transcribe
    segments, info = model.transcribe(
        input_file, 
        word_timestamps=True, 
        initial_prompt=None,
        beam_size=4, 
        language="zh", 
        max_new_tokens=192, 
        condition_on_previous_text=False,
        vad_filter=True, 
        vad_parameters=dict(min_silence_duration_ms=300)
    )
    
    process_segments(segments, output_file, segment_duration)

def process_segments(segments, output_file: str, segment_duration: float) -> None:
    # Process transcription segments
    txt_content = ""
    current_segment_start = 0.0
    current_segment_text = ""

    for segment in segments:
        start_time, end_time = segment.start, segment.end
        text = replace_special_chars(segment.text)
        
        current_segment_text += " " + text

        if float(end_time) - float(current_segment_start) >= segment_duration:
            formatted_segment = format_segment(current_segment_start, end_time, current_segment_text)
            print(formatted_segment)
            txt_content += formatted_segment
            
            current_segment_start = float(end_time)
            current_segment_text = ""
    
    # Handle the last segment
    if current_segment_text:
        formatted_segment = format_segment(current_segment_start, end_time, current_segment_text)
        print(formatted_segment)
        txt_content += formatted_segment
    
    # Write the result to file
    with open(output_file, 'w', encoding="utf-8") as txt_file:
        txt_file.write(txt_content)
    
    print(f"Saved: {os.path.abspath(output_file)}")

def format_segment(start_time: float, end_time: float, text: str) -> str:
    # Format a single segment
    start_time_str = format_to_custom_timestamp(start_time)
    end_time_str = format_to_custom_timestamp(end_time)
    return f"{start_time_str}-{end_time_str} {text.strip()}\n"

def replace_special_chars(text: str) -> str:
    # Replace special characters and correct common errors
    if text.startswith(("! ", " ")):
        text = text.lstrip("! ")
    
    replacements = {
        "XX": "OO", 
    }
    
    for original, replacement in replacements.items():
        text = text.replace(original, replacement)
    
    return text

def format_to_custom_timestamp(seconds: float) -> str:
    # Convert seconds to custom timestamp format
    dt = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=seconds)
    return f"{dt.hour:02d}:{dt.minute:02d}:{dt.second:02d}"

def main():
        
    # Preload models for each device
    device_indices = set([device_index for _, _, device_index in files])
    models = {}
    for device_index in device_indices:
        models[device_index] = WhisperModel(
            MODEL_PATH, device="cuda", device_index=device_index, compute_type="float16"
        )

    # Use thread pool to process audio files in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Pass the models here
        futures = [executor.submit(transcribe_audio, input_file, output_file, device_index, models[device_index]) 
                   for input_file, output_file, device_index in files]

        for future in concurrent.futures.as_completed(futures):
            future.result()

    print("All transcription tasks are completed.")

if __name__ == "__main__":
    main()
```

### Merging Transcription Texts

Simply copy and execute to merge the two transcribed documents directly (default filename is `merged_output.txt`). Renaming is not recommended to avoid issues during merging.

```python
def merge_transcriptions(file1, file2, output_file):
    def parse_timestamp(timestamp_str):
        """Convert timestamp in XX:XX:XX format to seconds"""
        h, m, s = map(int, timestamp_str.split(':'))
        return h * 3600 + m * 60 + s

    def format_timestamp(seconds):
        """Convert seconds to XX:XX:XX timestamp format"""
        return format_to_custom_timestamp(seconds)
    
    merged_content = ""
    total_duration = 0

    # Read the first file's content
    with open(file1, 'r', encoding="utf-8") as f1:
        for line in f1:
            # Assume timestamp is at the beginning of the line in XX:XX:XX-XX:XX:XX format
            time_range, text = line.split(' ', 1)
            start_time_str, end_time_str = time_range.split('-')
            
            # Convert timestamp to seconds
            start_time = parse_timestamp(start_time_str)
            end_time = parse_timestamp(end_time_str)
            
            # Update timestamps to include the cumulative total time
            new_start_time = format_timestamp(start_time + total_duration)
            new_end_time = format_timestamp(end_time + total_duration)
            
            # Merge the updated line
            merged_content += f"{new_start_time}-{new_end_time} {text}"
        
        # Update the cumulative total time
        total_duration = parse_timestamp(new_end_time)
    
    # Read the second file's content and merge
    with open(file2, 'r', encoding="utf-8") as f2:
        for line in f2:
            time_range, text = line.split(' ', 1)
            start_time_str, end_time_str = time_range.split('-')
            
            # Convert timestamp to seconds
            start_time = parse_timestamp(start_time_str)
            end_time = parse_timestamp(end_time_str)
            
            # Update timestamps to include the cumulative total time
            new_start_time = format_timestamp(start_time + total_duration)
            new_end_time = format_timestamp(end_time + total_duration)
            
            # Merge the updated line
            merged_content += f"{new_start_time}-{new_end_time} {text}"
    
    # Save the merged content to the output file
    with open(output_file, 'w', encoding="utf-8") as out_file:
        out_file.write(merged_content)

    print(f"Transcriptions merged and saved to {output_file}")

# Assume 04.txt and 05.txt have been successfully generated
merge_transcriptions("04.txt", "05.txt", "merged_output.txt")
```

## Contributing

Contributions are welcome in any form! If you need assistance or wish to make adjustments, feel free to consult GPT. I also developed this project gradually with its help~

## License

This project is licensed under the [MIT License](LICENSE). See the [LICENSE](LICENSE) file for details.

## Third-Party Licensing

- **faster-whisper**: This project uses the [faster-whisper](https://github.com/SYSTRAN/faster-whisper) library, which is licensed under the MIT License.
- **faster-whisper-large-v2-zh-TW Model**: The example model used in this project is sourced from [Hugging Face](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW). Please adhere to its [licensing terms](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW).

## Acknowledgements

Special thanks to the following projects and resources for supporting this project:

- [faster-whisper](https://github.com/SYSTRAN/faster-whisper) development team
- [Hugging Face](https://huggingface.co/XA9/faster-whisper-large-v2-zh-TW) for providing excellent model resources
- [Kaggle](https://www.kaggle.com/) for providing free GPU resources

## Contact

If you have any questions or suggestions, please reach out through the following methods:

- **Email**: a0953041880@gmail.com
- **GitHub Issues**: Not available

Thank you for your attention and support! See you next time~

---

> **Note**: This project makes full use of the free GPU resources provided by Kaggle. Please comply with Kaggle's terms of use, use resources responsibly, and avoid misuse.
