"""
!pip install faster-whisper==1.1.1 ctranslate2==4.4.0 -q
"""

"""
from faster_whisper import WhisperModel, BatchedInferencePipeline
import datetime, time, os, re, torch, glob
from typing import List, Tuple, Dict
import concurrent.futures, threading
"""

"""
# ---------- å¯èª¿åƒæ•¸ ----------
MODEL_PATH = "/kaggle/input/faster-whisper..."
AUDIO_ROOT = "/kaggle/input"            # åªæ”¹é€™è£¡å°±èƒ½æ›è³‡æ–™ä¾†æº
AUDIO_EXTS = (".wav", ".flac", ".mp3", ".ogg")   # å…è¨±çš„éŸ³æª”å‰¯æª”å
SEGMENT_DURATION = 30.0                          # æ¯æ®µæœ€é•·ç§’æ•¸
BATCH_SIZE = 8
MAX_CONCURRENCY_PER_GPU = 2                     # åŒå¼µå¡ä¸¦è¡Œä¸Šé™
REPLACEMENTS: Dict[str, str] = {                # å¸¸è¦‹éŒ¯å­—ä¿®æ­£è¡¨
    "XX": "OO" 
}
INITIAL_PROMPT = "æ³•å¾‹"                      # çµ¦æ¨¡å‹çš„ system promptï¼ˆå¯ç•™ç©ºï¼‰

# ---------- è‡ªå‹•æ”¶é›†éŸ³æª” ----------
def collect_audio_files(root: str, exts=AUDIO_EXTS) -> List[str]:
    """
    éè¿´èµ°è¨ª root åº•ä¸‹æ‰€æœ‰å­ç›®éŒ„ï¼Œ
    åªè¦æª”åå‰¯æª”åï¼ˆä¸è«–å¤§å°å¯«ï¼‰ç¬¦åˆ extsï¼Œå°±æ”¶é€²ä¾†ã€‚
    """
    exts_lower = {e.lower() for e in exts}
    files = []
    for dirpath, _, filenames in os.walk(root):
        for fn in filenames:
            ext = os.path.splitext(fn)[1].lower()
            if ext in exts_lower:
                files.append(os.path.join(dirpath, fn))
    return sorted(files)

# ---------- å»ºç«‹ (éŸ³æª”, è¼¸å‡ºæª”, GPU index) å°ç…§è¡¨ ----------
def create_job_table(audio_files: List[str], gpu_count: int) -> List[Tuple[str, str, int]]:
    jobs = []
    for idx, path in enumerate(audio_files, start=1):
        # å–åŸå§‹æª”åï¼ˆä¸å«å‰¯æª”åï¼‰ä½œç‚ºè¼¸å‡ºæª”å
        base = os.path.splitext(os.path.basename(path))[0]
        out_name = f"{base}.txt"
        gpu_idx = idx % gpu_count
        jobs.append((path, out_name, gpu_idx))
    return jobs

# ---------- å–ä»£/æ¸…æ´—å·¥å…· ----------
pattern = re.compile("|".join(re.escape(k) for k in REPLACEMENTS.keys()))
def clean_text(txt: str) -> str:
    txt = txt.lstrip("! ")
    return pattern.sub(lambda m: REPLACEMENTS[m.group(0)], txt)

def to_timestamp(sec: float) -> str:
    h = int(sec // 3600)
    m = int((sec % 3600) // 60)
    s = int(sec % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"

def fmt_chunk(start: float, end: float, txt: str) -> str:
    return f"{to_timestamp(start)}-{to_timestamp(end)} {txt.strip()}\n"

# ---------- è½‰éŒ„ + å¯«æª” ----------
def process_segments(segments, outfile: str, max_len=SEGMENT_DURATION):
    buf, chunk_start, chunk_txt = "", 0.0, ""
    for seg in segments:
        chunk_txt += " " + clean_text(seg.text)
        if seg.end - chunk_start >= max_len:
            line = fmt_chunk(chunk_start, seg.end, chunk_txt)
            print(line, end="", flush=True)
            buf += line
            chunk_start, chunk_txt = seg.end, ""
    if chunk_txt:
        line = fmt_chunk(chunk_start, seg.end, chunk_txt)
        print(line, end="", flush=True)
        buf += line
    with open(outfile, "w", encoding="utfâ€‘8") as fh:
        fh.write(buf)
    print(f" âœ” å·²å¯«å…¥ {outfile}")

def transcribe_single(job, pipelines, semaphores):
    in_path, out_path, gpu_idx = job
    sem = semaphores[gpu_idx]
    with sem:  # é™åˆ¶åŒä¸€å¼µå¡çš„ä¸¦è¡Œæ•¸
        try:
            segments, _info = pipelines[gpu_idx].transcribe(
                in_path,
                batch_size=BATCH_SIZE,
                word_timestamps=True,
                hallucination_silence_threshold=3,
                initial_prompt=INITIAL_PROMPT or None,
                beam_size=5,
                temperature=0,
                patience=1.5,
                language="zh",
                max_new_tokens=256,
                condition_on_previous_text=False,
                no_repeat_ngram_size=3,
                vad_filter=True,
                vad_parameters={"min_silence_duration_ms": 250, "speech_pad_ms": 600},
                log_progress=True,
            )
            process_segments(segments, out_path)
        except Exception as exc:
            print(f" âœ˜ è½‰éŒ„å¤±æ•—: {in_path} ({exc})")

# ---------- ä¸»æµç¨‹ ----------
def main():
    # 1. æª¢æŸ¥ GPU æ•¸é‡
    gpu_count = torch.cuda.device_count() or 1
    if gpu_count > 4:
        gpu_count = 4
    print(f"åµæ¸¬åˆ° GPU æ•¸é‡ï¼š{gpu_count}")

    # 2. æƒæéŸ³æª”
    audio_files = collect_audio_files(AUDIO_ROOT)
    if not audio_files:
        raise RuntimeError(f"æ‰¾ä¸åˆ°ä»»ä½•éŸ³æª”æ–¼ {AUDIO_ROOT}")
    print(f"å…±æ‰¾åˆ° {len(audio_files)} å€‹éŸ³æª”")

    # 3. å»ºç«‹å·¥ä½œæ¸…å–®ï¼ˆåŸæª”åè¼¸å‡ºï¼‰
    job_table = create_job_table(audio_files, gpu_count)

    # 4. åˆå§‹åŒ–æ¨¡å‹èˆ‡ pipeline
    pipelines = {}
    for idx in range(gpu_count):
        dev = "cuda" if torch.cuda.is_available() else "cpu"
        model = WhisperModel(MODEL_PATH, device=dev, device_index=idx, compute_type="float16")
        pipelines[idx] = BatchedInferencePipeline(model=model)
        print(f"GPU {idx} æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    # 5. å»ºç«‹ Semaphore æ§åˆ¶å–®å¡ä½µç™¼
    semaphores = {idx: threading.Semaphore(MAX_CONCURRENCY_PER_GPU) for idx in range(gpu_count)}

    # 6. å¤šåŸ·è¡Œç·’è½‰éŒ„
    total_workers = gpu_count * MAX_CONCURRENCY_PER_GPU
    with concurrent.futures.ThreadPoolExecutor(max_workers=total_workers) as pool:
        futures = [pool.submit(transcribe_single, job, pipelines, semaphores) for job in job_table]
        for _ in concurrent.futures.as_completed(futures):
            pass

    print("ğŸ‰ æ‰€æœ‰è½‰éŒ„ä»»å‹™å®Œæˆï¼")

# Entry
if __name__ == "__main__":
    audio_list = collect_audio_files(AUDIO_ROOT)
    print(f"å…±æ‰¾åˆ° {len(audio_list)} å€‹éŸ³æª”ï¼Œå‰ 10 ç­†ï¼š")
    for p in audio_list[:10]:
        print("  ", p)
    tic = time.time()
    main()
    print(f"ç¸½è€—æ™‚ï¼š{time.time() - tic:.1f} ç§’")
