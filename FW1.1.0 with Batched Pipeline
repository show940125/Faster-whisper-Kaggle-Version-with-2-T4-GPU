"""
%%time
# 升級至 faster-whisper 1.1.0 並安裝相依套件
!pip install faster-whisper==1.1.0 ctranslate2==4.4.0
"""

"""
from faster_whisper import WhisperModel, BatchedInferencePipeline
import datetime
import os
import concurrent.futures
from typing import List, Tuple
"""

"""
# 設定常量
MODEL_PATH = "/kaggle/input/faster-whisper-large-v2-zh-tw/transformers/default/2"
SEGMENT_DURATION = 30.0  # 每個段落的最大時長（秒）
MAX_WORKERS = 2          # 並行處理的最大線程數
BATCH_SIZE = 16          # 批量大小，可根據需要調整

# 定義多個音頻文件（格式：輸入文件路徑, 輸出文本文件名, 設備索引）
files: List[Tuple[str, str, int]] = [
    ("/kaggle/input/cpy01flac/CPY01_part1.flac", "04.txt", 0),
    ("/kaggle/input/cpy01flac/CPY01_part2.flac", "05.txt", 1)
]

def transcribe_audio(input_file: str, output_file: str, device_index: int, batched_model: BatchedInferencePipeline, segment_duration: float = SEGMENT_DURATION) -> None:
    """
    使用批量推理模型轉錄音頻文件，並將結果保存至指定的文本文件。
    """
    # 執行轉錄，啟用進度日誌
    segments, info = batched_model.transcribe(
        input_file,
        batch_size=BATCH_SIZE,
        word_timestamps=True,
        initial_prompt="ZH-TW",
        beam_size=5,
        language="zh",
        max_new_tokens=224,
        condition_on_previous_text=False,
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=500),
        log_progress=True  # 啟用轉錄進度顯示
    )
    
    # 處理並保存轉錄段落
    process_segments(segments, output_file, segment_duration)

def process_segments(segments, output_file: str, segment_duration: float) -> None:
    """
    處理轉錄結果的段落，將其格式化並寫入文本文件。
    """
    txt_content = ""
    current_segment_start = 0.0
    current_segment_text = ""

    for segment in segments:
        start_time, end_time = segment.start, segment.end
        text = replace_special_chars(segment.text)
        
        current_segment_text += " " + text

        if float(end_time) - float(current_segment_start) >= segment_duration:
            formatted_segment = format_segment(current_segment_start, end_time, current_segment_text)
            print(formatted_segment)
            txt_content += formatted_segment
            
            current_segment_start = float(end_time)
            current_segment_text = ""
    
    # 處理最後一個段落
    if current_segment_text:
        formatted_segment = format_segment(current_segment_start, end_time, current_segment_text)
        print(formatted_segment)
        txt_content += formatted_segment
    
    # 將結果寫入文件
    with open(output_file, 'w', encoding="utf-8") as txt_file:
        txt_file.write(txt_content)
    
    print(f"已保存: {os.path.abspath(output_file)}")

def format_segment(start_time: float, end_time: float, text: str) -> str:
    """
    將單個段落格式化為指定的時間戳格式。
    """
    start_time_str = format_to_custom_timestamp(start_time)
    end_time_str = format_to_custom_timestamp(end_time)
    return f"{start_time_str}-{end_time_str} {text.strip()}\n"

def replace_special_chars(text: str) -> str:
    """
    替換特殊字符並糾正常見錯誤。
    """
    if text.startswith(("! ", " ")):
        text = text.lstrip("! ")
    
    replacements = {
        "XX": "OO"
    }
    
    for original, replacement in replacements.items():
        text = text.replace(original, replacement)
    
    return text

def format_to_custom_timestamp(seconds: float) -> str:
    """
    將秒數轉換為自定義時間戳格式（HH:MM:SS）。
    """
    dt = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=seconds)
    return f"{dt.hour:02d}:{dt.minute:02d}:{dt.second:02d}"

def main():
    """
    主函數：預加載模型並並行處理音頻轉錄任務。
    """
    # 預加載每個設備的批量推理模型
    device_indices = set([device_index for _, _, device_index in files])
    batched_models = {}
    for device_index in device_indices:
        model = WhisperModel(
            MODEL_PATH, device="cuda", device_index=device_index, compute_type="float16"
        )
        batched_models[device_index] = BatchedInferencePipeline(model=model)
    
    # 使用線程池並行處理音頻文件
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # 提交轉錄任務，傳入批量推理模型
        futures = [
            executor.submit(
                transcribe_audio, 
                input_file, 
                output_file, 
                device_index, 
                batched_models[device_index]
            ) 
            for input_file, output_file, device_index in files
        ]
        
        # 等待所有轉錄任務完成
        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"轉錄任務失敗: {e}")
    
    print("所有轉錄任務已完成。")

if __name__ == "__main__":
    main()
"""
